{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1035, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "java does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f7421fce4582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.extraJavaOptions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dcard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36msetSystemProperty\u001b[0;34m(cls, key, value)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \"\"\"\n\u001b[1;32m    323\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1530\u001b[0m                 answer[proto.CLASS_FQN_START:], self._gateway_client)\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} does not exist in the JVM\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: java does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '16g')\n",
    "SparkContext.setSystemProperty('spark.executor.extraJavaOptions', '-verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps')\n",
    "sc = SparkContext(\"local[*]\", 'dcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dcard\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://192.168.2.12:27017/dcard.talk_posts\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://192.168.2.12:27017/dcard.talk_posts\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: integer (nullable = true)\n",
      " |-- anonymousDepartment: boolean (nullable = true)\n",
      " |-- anonymousSchool: boolean (nullable = true)\n",
      " |-- commentCount: integer (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- createdAt: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- forumAlias: string (nullable = true)\n",
      " |-- forumId: string (nullable = true)\n",
      " |-- forumName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hidden: boolean (nullable = true)\n",
      " |-- hiddenByAuthor: boolean (nullable = true)\n",
      " |-- likeCount: integer (nullable = true)\n",
      " |-- media: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- pinned: boolean (nullable = true)\n",
      " |-- replyId: integer (nullable = true)\n",
      " |-- replyTitle: string (nullable = true)\n",
      " |-- reportReason: string (nullable = true)\n",
      " |-- school: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- updatedAt: string (nullable = true)\n",
      " |-- withNickname: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|     school|count|\n",
      "+-----------+-----+\n",
      "|       新堡大學|    9|\n",
      "| 國立高雄海洋科技大學|   42|\n",
      "|        屏科大|  446|\n",
      "|      亞當森大學|    1|\n",
      "|       格魯斯特|    1|\n",
      "| 國立臺南護理專科學校|   22|\n",
      "|     香港浸會大學|    1|\n",
      "|    love♏的♎|    1|\n",
      "|         七七|    1|\n",
      "|   Ladycaca|    1|\n",
      "|       伊比爾喬|    1|\n",
      "|      理論型情聖|    1|\n",
      "|        OwO|    1|\n",
      "|💸挖壕溝的女子嘟🔨|    1|\n",
      "|         米米|    1|\n",
      "|     142小女紙|    2|\n",
      "|        紅心K|    1|\n",
      "|       新鮮な肝|    1|\n",
      "|       米蘭大學|    2|\n",
      "|  倫敦大學伯貝克學院|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"school\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id=6150, anonymousDepartment=False, anonymousSchool=False, commentCount=19, content='好酷噢可以按讚XD', createdAt='2014-04-10T08:16:24.673Z', department='資訊工程學系', excerpt='好酷噢可以按讚XD', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='閒聊', gender='M', hidden=False, hiddenByAuthor=False, likeCount=82, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='淡江大學', tags=[], title='新功能耶！', updatedAt='2014-04-10T08:16:24.673Z', withNickname=None),\n",
       " Row(_id=6151, anonymousDepartment=False, anonymousSchool=False, commentCount=60, content='大家快來給點建議吧 : )', createdAt='2014-04-10T09:15:11.945Z', department=' ', excerpt='大家快來給點建議吧 : )', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='閒聊', gender='D', hidden=False, hiddenByAuthor=False, likeCount=18, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='狄卡', tags=[], title='新版建議', updatedAt='2014-04-10T09:15:11.945Z', withNickname=None),\n",
       " Row(_id=6152, anonymousDepartment=False, anonymousSchool=False, commentCount=6, content='或用撰寫新文章回應打成長長一篇！', createdAt='2014-04-10T09:40:02.307Z', department='資訊管理學系', excerpt='或用撰寫新文章回應打成長長一篇！', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='閒聊', gender='M', hidden=False, hiddenByAuthor=False, likeCount=5, media=[], pinned=False, replyId=6151, replyTitle='新版建議', reportReason='', school='國立臺灣大學', tags=[], title='Re: 新版建議', updatedAt='2014-04-10T09:40:02.307Z', withNickname=None),\n",
       " Row(_id=6171, anonymousDepartment=False, anonymousSchool=False, commentCount=20, content='個人覺得喜歡的按鈕功能很惹人煩\\n不需要上個網全世界都要按讚吧QAQ\\n不認為喜歡鈕的新增是個好主意...', createdAt='2014-04-12T13:36:16.320Z', department='地球科學系', excerpt='個人覺得喜歡的按鈕功能很惹人煩\\n不需要上個網全世界都要按讚吧QAQ\\n不認為喜歡鈕的新增是個好主意...', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='閒聊', gender='M', hidden=False, hiddenByAuthor=False, likeCount=27, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='中央大學', tags=[], title='拜託不要越來越像臉書...', updatedAt='2014-04-12T13:36:16.320Z', withNickname=None),\n",
       " Row(_id=6181, anonymousDepartment=False, anonymousSchool=False, commentCount=4, content='https://www.youtube.com/watch?v=JAjXKnFoaZI\\n\\n2014臺大畢業季主題曲「還記得嗎」\\n\\n詞：Crispy脆樂團\\n曲：Skippy\\n\\n這條路我走了四年 總仰望盡頭的藍天\\n兩排的椰子樹就像侍衛 守護著我們的從前\\n總有一天我們要說再見 走入大人的世界\\n倒影在湖面上的時間 又悄悄走了幾圈\\n\\n還記得嗎 對爸媽說我會拿書卷獎\\n在地下室等著迎接曙光\\n看見你的表情和我一樣\\n我笑了吧\\n\\n還記得嗎 鏡子裡整齊劃一的模樣\\n一個人躲在房間練吉他\\n拼了命跟上大家的步伐\\n我大聲唱\\n\"時間不能夠重來 我不要留下一分一秒的空白\"\\n\\n你對我說面對新的生活 有些不安和惶恐\\n是會一飛沖天變成英雄 還是流星般墜落\\n別害怕 相信自己 和那些倔強執著\\n每個人都是一片天空 綻放自己的煙火\\n\\n還記得嗎 一起翹課占領籃球場\\n在宿舍電腦裡釋放魔法\\n在哪裡跌倒哪裡受了傷\\n我不會怕\\n\\n還記得嗎 一起走在圖書館的長廊\\n在耳邊小小聲說著夢想\\n空無一人的椰林大道上\\n我大聲唱\\n\"想要改變全世界 這裡就是我們的 起點\"\\n\\n還記得嗎 第一次遇見她的地方\\n牽著手散步在舟山路上\\n不再去在意別人的眼光\\n我吻了她\\n\\n還記得嗎 對鏡子告訴自己要堅強\\n不管未來會在哪個地方\\n在人擠人的畢業舞會上\\n換上西裝\\n\"人生不需要100分 能和你們一起走過就值得\"', createdAt='2014-04-13T03:30:48.722Z', department=' ', excerpt='https://www.youtube.com/watch?v=JAjXKnFoaZI\\n\\n2014臺大畢業季主題曲「還記得嗎」\\n\\n詞：Crispy脆樂團\\n曲：Skippy\\n\\n這條路我走了四年 總仰望盡', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='閒聊', gender='D', hidden=False, hiddenByAuthor=False, likeCount=26, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='狄卡', tags=[], title='2014臺大畢業季主題曲「還記得嗎」MV', updatedAt='2014-04-13T03:30:48.722Z', withNickname=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "content = df.select('content')\n",
    "print(content.rdd.getNumPartitions())\n",
    "content_rdd = content.rdd.repartition(16).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81525\n"
     ]
    }
   ],
   "source": [
    "lineLengths = content_rdd.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_url_and_punctuation(sentence):\n",
    "    # remove url\n",
    "    if 'http' in sentence:\n",
    "        sentence = re.sub(r'[a-zA-Z0-9_/:.]', '', sentence, flags=re.MULTILINE)\n",
    "\n",
    "    # remove punctuation\n",
    "    text_list = re.split('\\W+', sentence)\n",
    "    return list(filter(None, text_list))\n",
    "\n",
    "def to_ngrams(unigrams, length):\n",
    "    return Counter(zip(*[unigrams[i:] for i in range(length)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = '🤔剛剛要回文時已經找不到了\\n總之，內容就是跟姊姊一起刮卡然後中了205萬，於是發上來的照片我怎麼看都沒中😂。\\n\\n祝大家新年快樂，也希望大家在刮彩卷之類的東西能夠好好看清楚喔🙌，以免遇到以為中獎一家人很high，結果被網友說沒中的哭哭戲碼。\\nhttps://i.imgur.com/Svbg4BF.jpg\\n沒有截文章 截到圖片而已\\n\\n大家加油喔👊'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['剛剛要回文時已經找不到了',\n",
       " '總之',\n",
       " '內容就是跟姊姊一起刮卡然後中了萬',\n",
       " '於是發上來的照片我怎麼看都沒中',\n",
       " '祝大家新年快樂',\n",
       " '也希望大家在刮彩卷之類的東西能夠好好看清楚喔',\n",
       " '以免遇到以為中獎一家人很',\n",
       " '結果被網友說沒中的哭哭戲碼',\n",
       " '沒有截文章',\n",
       " '截到圖片而已',\n",
       " '大家加油喔']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url_and_punctuation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result = content_rdd.map(lambda a: remove_url_and_punctuation(a['content'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.78852820396423\n"
     ]
    }
   ],
   "source": [
    "# compute unigram and bigram count\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "result = content_rdd.map(lambda a: remove_url_and_punctuation(a['content'])).collect()\n",
    "\n",
    "for post in result:\n",
    "    for line in post:\n",
    "        unigram_counter.update(line)\n",
    "        bigram_counter.update(to_ngrams(line, 2))\n",
    "        trigram_counter.update(to_ngrams(line, 3))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('的', 555164), ('我', 355176), ('是', 336489), ('不', 258415), ('一', 244291), ('有', 243244), ('了', 175684), ('人', 147514), ('在', 140303), ('到', 127492)]\n",
      "[(('沒', '有'), 44669), (('什', '麼'), 42759), (('可', '以'), 40630), (('一', '個'), 38825), (('自', '己'), 38158), (('知', '道'), 37187), (('大', '家'), 36745), (('覺', '得'), 35109), (('因', '為'), 34636), (('真', '的'), 33224)]\n",
      "[(('不', '知', '道'), 18890), (('的', '時', '候'), 15379), (('有', '沒', '有'), 13914), (('為', '什', '麼'), 9675), (('自', '己', '的'), 8395), (('_', '_', '_'), 7672), (('哈', '哈', '哈'), 7248), (('我', '覺', '得'), 6490), (('是', '不', '是'), 6301), (('真', '的', '很'), 6168)]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_counter.most_common(10))\n",
    "print(bigram_counter.most_common(10))\n",
    "print(trigram_counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_to_three_grams(line):\n",
    "    print(line)\n",
    "    result = (Counter(line), to_ngrams(line, 2), to_ngrams(line, 3))\n",
    "    print(len(result[0]), len(result[1]), len(result[2]))\n",
    "    return result\n",
    "#     return (Counter(line), to_ngrams(line, 2), to_ngrams(line, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(content='🤔🤔\\n聽說今年在屏東某地的潮X高中\\n全國繁星第一 (110人)\\n但只有46個人上國立\\n難道這就是所謂有學校就讀的概念嗎?\\n\\n還有據說繁星進大學的 都蠻優秀的\\n是這樣嗎？')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['聽說今年在屏東某地的潮X高中',\n",
       " '全國繁星第一',\n",
       " '110人',\n",
       " '但只有46個人上國立',\n",
       " '難道這就是所謂有學校就讀的概念嗎',\n",
       " '還有據說繁星進大學的',\n",
       " '都蠻優秀的',\n",
       " '是這樣嗎']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(content_rdd.top(1))\n",
    "remove_url_and_punctuation(content_rdd.top(1)[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15678028\n"
     ]
    }
   ],
   "source": [
    "lineLengths = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# sec = int(round(time.time()))\n",
    "# print(sec)\n",
    "# sample_rdd = content_rdd.sample(False, 0.01, sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda line: one_to_three_grams(line)).reduce(lambda a, b: tuple(map(operator.add, a, b)))\n",
    "\n",
    "# result[0].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result_one_grams = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda s: Counter(s)).reduce(lambda a,b: a + b)\n",
    "\n",
    "# result_one_grams.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Utilizing spark Accumulator to calculate n-grams.\n",
    "# list_data = content_rdd.flatMap(lambda a: remove_url(a['content'])).collect()\n",
    "# result_list = sc.parallelize(list_data)\n",
    "# result_list.top(10)\n",
    "\n",
    "# class CounterAccumulatorParam(AccumulatorParam):\n",
    "#     def zero(self, initialValue):\n",
    "#         return initialValue\n",
    "\n",
    "#     def addInPlace(self, v1, v2):\n",
    "#         v1 += v2\n",
    "#         return v1\n",
    "\n",
    "# # Then, create an Accumulator of this type:\n",
    "# one_gram_accum = sc.accumulator(Counter(), CounterAccumulatorParam())\n",
    "# two_gram_accum = sc.accumulator(Counter(), CounterAccumulatorParam())\n",
    "# three_gram_accum = sc.accumulator(Counter(), CounterAccumulatorParam())\n",
    "\n",
    "# def one_to_three_grams_accum(line):\n",
    "#     one_gram_accum.add(Counter(line))\n",
    "#     two_gram_accum.add(to_ngrams(line, 2))\n",
    "#     three_gram_accum.add(to_ngrams(line, 3))\n",
    "\n",
    "# result_list.foreach(lambda line: one_to_three_grams_accum(line))\n",
    "\n",
    "# one_gram_accum.value.most_common(10)\n",
    "\n",
    "# two_gram_accum.value.most_common(10)\n",
    "\n",
    "# three_gram_accum.value.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " # Good-Turing Smoothing Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "V1 = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda s: len(s)).reduce(lambda a, b: a + b)\n",
    "V2 = V1 ** 2\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15670476 245800561178671\n"
     ]
    }
   ],
   "source": [
    "from math import log10\n",
    "# compute N1, N2, N3...\n",
    "unigram_Nr = Counter(unigram_counter.values())\n",
    "bigram_Nr = Counter(bigram_counter.values())\n",
    "# compute N0\n",
    "unigram_Nr[0] = V1 - len(unigram_counter)\n",
    "bigram_Nr[0] = V2 - len(bigram_counter)\n",
    "print(unigram_Nr[0], bigram_Nr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.942992669782335e-05, 1.0183823529411764, 1.6570397111913358, 3.281045751633987, 3.545816733067729, 5.52808988764045, 5.378048780487805, 7.428571428571429, 8.461538461538462, 9.272727272727273]\n",
      "[1.48258408464375e-09, 0.6455353712749026, 1.5699225491613036, 2.507757542280636, 3.497926924073594, 4.4898322035781755, 5.4324324324324325, 6.407449929837989, 7.322847813968305, 8.6522462562396]\n"
     ]
    }
   ],
   "source": [
    "# compute r\n",
    "unigram_r = [(i+1) * unigram_Nr[i+1] / unigram_Nr[i] for i in range(k)]\n",
    "bigram_r = [(i+1) * bigram_Nr[i+1] / bigram_Nr[i] for i in range(k)]\n",
    "print(unigram_r)\n",
    "print(bigram_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15678028 13901455\n"
     ]
    }
   ],
   "source": [
    "# compute normalize factor\n",
    "# compute N\n",
    "unigram_N = sum(unigram_counter.values())\n",
    "bigram_N = sum(bigram_counter.values())\n",
    "print(unigram_N, bigram_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15679048 13989855\n"
     ]
    }
   ],
   "source": [
    "# compute new probability sum\n",
    "unigram_N_ = unigram_N + k * unigram_Nr[k]\n",
    "bigram_N_ = bigram_N + k * bigram_Nr[k]\n",
    "print(unigram_N_, bigram_N_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999349450298258 0.9936811353655917\n"
     ]
    }
   ],
   "source": [
    "# normalize factor: N/N’\n",
    "unigram_norm_factor = unigram_N / unigram_N_\n",
    "bigram_norm_factor = bigram_N / bigram_N_\n",
    "print(unigram_norm_factor, bigram_norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Estimating P(w) and P(w’|w)\n",
    "def prob_1word(unigram):\n",
    "    count = unigram_counter[unigram]\n",
    "    r = unigram_r[count] if count < k else count\n",
    "    return log10(r / unigram_N_)\n",
    "def prob_2words(text_front, text_rear):\n",
    "    count = bigram_counter[text_front, text_rear]\n",
    "    r = bigram_r[count] if count < k else count\n",
    "    return log10(r / bigram_N_)\n",
    "def prob_word_by_word(text_front, text_rear):\n",
    "    return prob_2words(text_front, text_rear) - prob_1word(text_front)\n",
    "def prob_words(words):\n",
    "    return prob_1word(words[0]) + sum(prob_word_by_word(words[i-1], words[i]) for i in range(1, len(words)))\n",
    "def prob_text(text):\n",
    "    return prob_words(text.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.352024606932168\n",
      "-5.231999360819604\n",
      "-1.8624478784734173\n"
     ]
    }
   ],
   "source": [
    "print(prob_1word('清'))\n",
    "print(prob_2words('清','華'))\n",
    "print(prob_word_by_word('我','很'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unicount_log = {k: log10(v) for k, v in unigram_counter.items()}\n",
    "bicount_log = {k: log10(v) for k, v in bigram_counter.items()}\n",
    "unigram_r_log = [log10(r) for r in unigram_r]\n",
    "bigram_r_log = [log10(r) for r in bigram_r]\n",
    "unigram_N_log = log10(unigram_N_)\n",
    "bigram_N_log = log10(bigram_N_)\n",
    "\n",
    "\n",
    "def prob_1word(unigram):\n",
    "    count = unigram_counter[unigram]\n",
    "    r = unigram_r_log[count] if count < k else unicount_log[unigram]\n",
    "    return r - unigram_N_log\n",
    "def prob_2words(text_front, text_rear):\n",
    "    count = bigram_counter[text_front, text_rear]\n",
    "    r = bigram_r_log[count] if count < k else bicount_log[text_front, text_rear]\n",
    "    return r - bigram_N_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.3520246069321686\n",
      "-3.7596345517270455\n",
      "-5.231999360819604\n",
      "-1.879974753887435\n"
     ]
    }
   ],
   "source": [
    "print(prob_1word(u'清'))\n",
    "print(prob_1word(u'華'))\n",
    "print(prob_2words(u'清', u'華'))\n",
    "print(prob_word_by_word(u'清',u'華'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# N_unigram_corpus = math.log2(float(sum(unigram_counter.values())))\n",
    "# N_bigram_corpus = math.log2(float(sum(bigram_counter.values())))\n",
    "def pmi(words):\n",
    "    word1 = words[0]\n",
    "    word2 = words[1]\n",
    "    # Good-Turing Estimation 將次數小於k的字做一些調整 目標不要讓沒出現過的字 機率為0\n",
    "    count_word1 = unigram_counter[word1]\n",
    "    r_word1 = unigram_r_log[count_word1] if count_word1 < k else unicount_log[word1]\n",
    "    count_word2 = unigram_counter[word2]\n",
    "    r_word2 = unigram_r_log[count_word2] if count_word2 < k else unicount_log[word2]\n",
    "    count_word1_and_word2 = bigram_counter[(word1, word2)]\n",
    "    r_word1_and_word2 = bigram_r_log[count_word1_and_word2] if count_word1_and_word2 < k else bicount_log[(word1, word2)]\n",
    "\n",
    "    # mutual information algorithm\n",
    "    prob_word1 = r_word1 - unigram_N_log\n",
    "    prob_word2 = r_word2 -  unigram_N_log\n",
    "    prob_word1_word2 = r_word1_and_word2 -  bigram_N_log\n",
    "    return prob_word1_word2 - (prob_word1+prob_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.282093830811121\n",
      "0.6955342288888389\n",
      "1.5351483023990387\n",
      "2.5625752923560103\n",
      "0.632807350998914\n"
     ]
    }
   ],
   "source": [
    "print(pmi((u'聰', u'思')))\n",
    "print(pmi((u'很',u'開')))\n",
    "print(pmi((u'開', u'心')))\n",
    "print(pmi((u'吃', u'飯')))\n",
    "print(pmi((u'我', u'弟')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "threshold = 0.5\n",
    "\n",
    "def word_segmentation(sentence):\n",
    "    # input : 忘了當初選的選項  \n",
    "    \n",
    "    # Handle empty sentence.\n",
    "    if len(sentence)==0:\n",
    "        return sentence\n",
    "    words_list = to_words(sentence, 2)\n",
    "    max_probability_dict = find_max_prob(to_prob_dict(words_list))\n",
    "    sentences = seperate_sentence(sentence, max_probability_dict)\n",
    "    return sentences.split()\n",
    "\n",
    "def to_words(unigrams, length):\n",
    "    return list(zip(*[unigrams[i:] for i in range(length)]))\n",
    "\n",
    "\n",
    "def to_prob_dict(words_list):\n",
    "#   input : [('忘', '了'), ('了', '當'), ('當', '初'), ('初', '選'), ('選', '的'), ('的', '選'), ('選', '項')]\n",
    "    result_sentence = {}\n",
    "    for word in words_list:\n",
    "        result_sentence[word] = pmi(word)\n",
    "    return result_sentence\n",
    "\n",
    "\n",
    "def find_max_prob(probability_dict):\n",
    "    sorted_prob = sorted(probability_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#     print(sorted_prob)  # Can see every candidate words prob.\n",
    "    result_list = []\n",
    "    for candidate in sorted_prob:\n",
    "        prob = candidate[1]\n",
    "        words_tuple = candidate[0]\n",
    "        if prob > threshold:\n",
    "            result_list.append(''.join(map(str, words_tuple)))  # ('選', '項') => 選項\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def seperate_sentence(orginal_sentence, max_probability_dict):\n",
    "    segment_word = orginal_sentence\n",
    "    for candidate in max_probability_dict:\n",
    "        insert_word = \" \"+candidate+\" \"\n",
    "        segment_word = segment_word.replace(candidate, insert_word)\n",
    "    return segment_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '已經', '向', '六個', '女生', '邀請', '她們', '當我', '舞伴']\n"
     ]
    }
   ],
   "source": [
    "test = word_segmentation(u'我已經向六個女生邀請她們當我舞伴')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_lines = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "呃\n",
      "['呃']\n",
      "\n",
      "忘了當初選的選項\n",
      "['忘了', '當初', '選的', '選項']\n",
      "\n",
      "想要知道的話可以顯示嗎\n",
      "['想要', '知道', '的話', '可以', '顯示', '嗎']\n",
      "\n",
      "我猜線在是不行\n",
      "['我猜', '線在是', '不行']\n",
      "\n",
      "要改了系統才能吧\n",
      "['要改', '了', '系統', '才能', '吧']\n",
      "\n",
      "有時候難得遇到認識的人其實很高興\n",
      "['有', '時候', '難得', '遇到', '認識', '的人', '其實', '很', '高興']\n",
      "\n",
      "不過因為太久沒看到\n",
      "['不過', '因為', '太久', '沒', '看到']\n",
      "\n",
      "不知道要說什麼\n",
      "['不', '知道', '要說', '什麼']\n",
      "\n",
      "所以就快步離開了\n",
      "['所以', '就', '快步', '離開', '了']\n",
      "\n",
      "大家都有這樣的經驗嗎\n",
      "['大家', '都有', '這樣', '的', '經驗', '嗎']\n",
      "\n",
      "感覺淡江的同學好多阿\n",
      "['感覺', '淡江', '的', '同學', '好多', '阿']\n",
      "\n",
      "抽到好多都是淡江的\n",
      "['抽到', '好多', '都是', '淡江', '的']\n",
      "\n",
      "好友也都是\n",
      "['好友', '也', '都是']\n",
      "\n",
      "學校即將舉行耶誕舞會\n",
      "['學校', '即將', '舉行', '耶誕', '舞會']\n",
      "\n",
      "我已經向六個女生邀請她們當我舞伴\n",
      "['我', '已經', '向', '六個', '女生', '邀請', '她們', '當我', '舞伴']\n",
      "\n",
      "但是都被拒絕\n",
      "['但是', '都被', '拒絕']\n",
      "\n",
      "我很難過\n",
      "['我很', '難過']\n",
      "\n",
      "我無法再承受任何打擊\n",
      "['我', '無法', '再', '承受', '任何', '打擊']\n",
      "\n",
      "我心理系同學對我說\n",
      "['我', '心理', '系', '同學', '對', '我說']\n",
      "\n",
      "不要難過\n",
      "['不要', '難過']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in total_lines[:20]:\n",
    "    print(sentence)\n",
    "    print(word_segmentation(sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def partial_match(word, counter):\n",
    "    new_counter = Counter()\n",
    "    for key, value in counter.most_common():\n",
    "        if all(k1 == k2 or k2 is None for k1, k2 in zip(key, word)):\n",
    "            new_counter[key] = value\n",
    "    return new_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "one_match = partial_match((u'沒', None), bigram_counter)\n",
    "two_match = partial_match((u'沒', u'有', None), trigram_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('沒', '有'), 44669),\n",
       " (('沒', '什'), 2285),\n",
       " (('沒', '想'), 2201),\n",
       " (('沒', '辦'), 2084),\n",
       " (('沒', '人'), 1754),\n",
       " (('沒', '看'), 1645),\n",
       " (('沒', '關'), 1577),\n",
       " (('沒', '事'), 1472),\n",
       " (('沒', '錯'), 1344),\n",
       " (('沒', '多'), 864)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_match.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('沒', '有', '人'), 4608),\n",
       " (('沒', '有', '什'), 1272),\n",
       " (('沒', '有', '推'), 838),\n",
       " (('沒', '有', '這'), 832),\n",
       " (('沒', '有', '很'), 806),\n",
       " (('沒', '有', '要'), 713),\n",
       " (('沒', '有', '一'), 705),\n",
       " (('沒', '有', '想'), 627),\n",
       " (('沒', '有', '任'), 504),\n",
       " (('沒', '有', '看'), 495)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_match.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/data/mllib/sample_lda_data.txt MapPartitionsRDD[37] at textFile at NativeMethodAccessorImpl.java:0\n",
      "PythonRDD[38] at RDD at PythonRDD.scala:48\n",
      "PythonRDD[40] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the data\n",
    "data = sc.textFile(\"/usr/local/spark/data/mllib/sample_lda_data.txt\")\n",
    "print(data)\n",
    "parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "print(parsedData)\n",
    "# Index documents with unique IDs\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 11 words):\n",
      "Topic 0:\n",
      " 9.83124703324\n",
      " 6.26635606566\n",
      " 2.28796463598\n",
      " 10.3650763441\n",
      " 6.11907922619\n",
      " 8.32748840998\n",
      " 14.8422943338\n",
      " 2.51740931974\n",
      " 3.12135581995\n",
      " 9.55781644441\n",
      " 13.0525672405\n",
      "Topic 1:\n",
      " 7.3914021596\n",
      " 9.62750309174\n",
      " 7.80180750758\n",
      " 12.3654287092\n",
      " 7.20838108364\n",
      " 3.59494206471\n",
      " 8.94472034523\n",
      " 1.78870314191\n",
      " 3.34569455641\n",
      " 6.7349073487\n",
      " 17.6358217628\n",
      "Topic 2:\n",
      " 8.77735080716\n",
      " 13.1061408426\n",
      " 1.91022785644\n",
      " 17.2694949467\n",
      " 11.6725396902\n",
      " 10.0775695253\n",
      " 7.21298532093\n",
      " 5.69388753835\n",
      " 1.53294962363\n",
      " 7.70727620689\n",
      " 2.31161099666\n"
     ]
    }
   ],
   "source": [
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3)\n",
    "\n",
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n",
    "      + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, ldaModel.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "num_of_stop_words = 50      # Number of most common words to remove, trying to eliminate stop words\n",
    "num_topics = 3              # Number of topics we are looking for\n",
    "num_words_per_topic = 10    # Number of words to display for each topic\n",
    "max_iterations = 35         # Max number of times to iterate before finishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('沒', '有'), 44669),\n",
       " (('什', '麼'), 42759),\n",
       " (('可', '以'), 40630),\n",
       " (('一', '個'), 38825),\n",
       " (('自', '己'), 38158),\n",
       " (('知', '道'), 37187),\n",
       " (('大', '家'), 36745),\n",
       " (('覺', '得'), 35109),\n",
       " (('因', '為'), 34636),\n",
       " (('真', '的'), 33224),\n",
       " (('我', '們'), 31995),\n",
       " (('不', '是'), 28781),\n",
       " (('所', '以'), 28237),\n",
       " (('朋', '友'), 27289),\n",
       " (('就', '是'), 26864),\n",
       " (('時', '候'), 26351),\n",
       " (('還', '是'), 26081),\n",
       " (('這', '樣'), 24335),\n",
       " (('不', '知'), 21375),\n",
       " (('然', '後'), 21356),\n",
       " (('看', '到'), 21188),\n",
       " (('的', '人'), 20874),\n",
       " (('我', '的'), 20783),\n",
       " (('是', '我'), 20324),\n",
       " (('有', '人'), 19600),\n",
       " (('怎', '麼'), 19496),\n",
       " (('現', '在'), 18760),\n",
       " (('但', '是'), 18669),\n",
       " (('他', '們'), 18649),\n",
       " (('的', '時'), 18577),\n",
       " (('如', '果'), 17887),\n",
       " (('一', '下'), 17608),\n",
       " (('不', '會'), 16947),\n",
       " (('這', '個'), 16427),\n",
       " (('開', '始'), 16150),\n",
       " (('喜', '歡'), 15823),\n",
       " (('很', '多'), 15611),\n",
       " (('哈', '哈'), 15334),\n",
       " (('一', '直'), 15143),\n",
       " (('都', '是'), 14362),\n",
       " (('一', '樣'), 14308),\n",
       " (('個', '人'), 14118),\n",
       " (('有', '沒'), 14033),\n",
       " (('不', '要'), 13770),\n",
       " (('比', '較'), 13766),\n",
       " (('有', '一'), 13737),\n",
       " (('了', '一'), 13654),\n",
       " (('最', '近'), 13521),\n",
       " (('是', '不'), 13367),\n",
       " (('今', '天'), 13314)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sc.parallelize(result)\n",
    "unigram_counter.most_common(50)\n",
    "bigram_counter.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def document_segmentation(document):\n",
    "    sentences = remove_url_and_punctuation(document)  # output ['呃', '忘了當初選的選項', '想要知道的話可以顯示嗎', '我猜線在是不行', '要改了系統才能吧']\n",
    "    \n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        # Handle empty sentence.\n",
    "        if len(sentence)==0:\n",
    "            return sentence\n",
    "        words_list = to_words(sentence, 2)\n",
    "        max_probability_dict = find_max_prob(to_prob_dict(words_list))\n",
    "        sentences = seperate_sentence(sentence, max_probability_dict)\n",
    "        sentence_list = sentence_list + sentences.split()\n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "# Convert the given document into a vector of word counts\n",
    "def document_vector(document):\n",
    "    id = document[1]\n",
    "    counts = Counter()\n",
    "    for token in document[0]:\n",
    "        if token in vocabulary:\n",
    "            token_id = vocabulary[token]\n",
    "            counts[token_id] += 1\n",
    "    counts = sorted(counts.items())\n",
    "    if len(counts) > 0 :\n",
    "        keys = [x[0] for x in counts]\n",
    "        values = [x[1] for x in counts]\n",
    "    else:\n",
    "        keys = [0]\n",
    "        values = [0]\n",
    "    return (id, Vectors.sparse(len(vocabulary), keys, values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_id=10003, content='呃....忘了當初選的選項，想要知道的話可以顯示嗎?\\r\\n我猜線在是不行...要改了系統才能吧?\\r\\n')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = df.select(df['_id'], df['content'])\n",
    "content_rdd = content.rdd.repartition(16).cache()\n",
    "# 把原本dcard的 id與標題 做比對\n",
    "\n",
    "\n",
    "content_rdd.take(1)[0]\n",
    "# result_rdd.map(lambda sentence: word_segmentation(sentence)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result_rdd = content_rdd.map(lambda posts: document_segmentation(posts['content'])).filter(lambda documnet: len(documnet)>0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['呃',\n",
       "  '忘了',\n",
       "  '當初',\n",
       "  '選的',\n",
       "  '選項',\n",
       "  '想要',\n",
       "  '知道',\n",
       "  '的話',\n",
       "  '可以',\n",
       "  '顯示',\n",
       "  '嗎',\n",
       "  '我猜',\n",
       "  '線在是',\n",
       "  '不行',\n",
       "  '要改',\n",
       "  '了',\n",
       "  '系統',\n",
       "  '才能',\n",
       "  '吧']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentence_counts = result_rdd.flatMap(lambda document: document) \\\n",
    "    .map(lambda word: (word, 1) if word is not list else ('', 0)) \\\n",
    "    .reduceByKey( lambda x,y: x + y) \\\n",
    "    .map(lambda tuple: (tuple[1], tuple[0])) \\\n",
    "    .sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Identify a threshold to remove the top words, in an effort to remove stop words\n",
    "threshold_value = sentence_counts.take(num_of_stop_words)[num_of_stop_words - 1][0]\n",
    "\n",
    "# Only keep words with a count less than the threshold identified above, and then index each one and collect them into a map\n",
    "vocabulary = sentence_counts                    \\\n",
    "    .filter(lambda x : x[0] < threshold_value)  \\\n",
    "    .map(lambda x: x[1])                        \\\n",
    "    .zipWithIndex()                             \\\n",
    "    .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "documents = result_rdd.zipWithIndex().map(document_vector).map(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inv_voc = {value: key for (key, value) in vocabulary.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  SparseVector(370239, {33: 1.0, 92: 1.0, 100: 1.0, 532: 1.0, 559: 1.0, 723: 1.0, 892: 1.0, 1043: 1.0, 1110: 1.0, 1635: 1.0, 2763: 1.0, 4281: 1.0, 8906: 1.0, 12721: 1.0, 188274: 1.0})],\n",
       " [1,\n",
       "  SparseVector(370239, {7: 1.0, 22: 1.0, 56: 1.0, 127: 1.0, 144: 1.0, 192: 1.0, 209: 1.0, 480: 1.0, 2107: 1.0, 2194: 1.0, 2411: 1.0, 2903: 1.0, 18201: 1.0})],\n",
       " [2,\n",
       "  SparseVector(370239, {14: 2.0, 31: 1.0, 35: 1.0, 107: 1.0, 506: 1.0, 687: 2.0, 853: 1.0, 3923: 2.0})],\n",
       " [3,\n",
       "  SparseVector(370239, {27: 1.0, 30: 1.0, 34: 1.0, 35: 1.0, 67: 1.0, 77: 1.0, 127: 1.0, 152: 1.0, 180: 1.0, 189: 1.0, 378: 1.0, 387: 2.0, 503: 2.0, 505: 2.0, 583: 1.0, 663: 2.0, 683: 1.0, 903: 1.0, 953: 1.0, 1052: 1.0, 1065: 1.0, 1241: 1.0, 1400: 1.0, 1745: 1.0, 1851: 2.0, 1877: 1.0, 2142: 1.0, 2498: 1.0, 2565: 1.0, 2896: 1.0, 3404: 1.0, 3929: 1.0, 5423: 1.0, 5460: 1.0, 8728: 1.0, 9388: 1.0, 9487: 1.0, 10895: 2.0, 12669: 1.0, 15978: 1.0, 23230: 1.0, 80429: 1.0, 153157: 1.0})],\n",
       " [4,\n",
       "  SparseVector(370239, {1: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, 17: 1.0, 34: 3.0, 40: 3.0, 63: 1.0, 70: 1.0, 77: 1.0, 94: 1.0, 123: 2.0, 125: 1.0, 146: 1.0, 149: 1.0, 151: 1.0, 161: 1.0, 189: 1.0, 193: 1.0, 201: 1.0, 218: 1.0, 221: 1.0, 227: 2.0, 264: 1.0, 280: 3.0, 316: 1.0, 325: 1.0, 331: 1.0, 387: 1.0, 388: 1.0, 404: 1.0, 462: 1.0, 463: 2.0, 532: 1.0, 550: 2.0, 573: 4.0, 579: 1.0, 598: 1.0, 599: 1.0, 614: 2.0, 615: 2.0, 620: 1.0, 621: 2.0, 639: 1.0, 652: 1.0, 657: 1.0, 666: 1.0, 673: 1.0, 738: 1.0, 825: 1.0, 964: 1.0, 1007: 1.0, 1030: 1.0, 1067: 1.0, 1075: 1.0, 1092: 1.0, 1145: 1.0, 1159: 1.0, 1193: 3.0, 1250: 1.0, 1271: 1.0, 1321: 1.0, 1382: 1.0, 1442: 1.0, 1538: 1.0, 1552: 1.0, 1575: 1.0, 1622: 1.0, 1628: 1.0, 1634: 1.0, 1714: 1.0, 1717: 1.0, 1736: 2.0, 1766: 1.0, 1777: 5.0, 1792: 1.0, 1852: 1.0, 1853: 2.0, 1894: 1.0, 1942: 1.0, 1999: 1.0, 2006: 1.0, 2022: 1.0, 2205: 1.0, 2247: 2.0, 2291: 1.0, 2349: 1.0, 2376: 2.0, 2499: 1.0, 2528: 1.0, 2631: 1.0, 2716: 9.0, 2755: 1.0, 2761: 1.0, 2764: 1.0, 2774: 4.0, 2900: 4.0, 2949: 2.0, 3039: 1.0, 3147: 1.0, 3249: 1.0, 3415: 1.0, 3435: 2.0, 3443: 1.0, 3458: 1.0, 3530: 1.0, 3628: 1.0, 3785: 1.0, 3837: 1.0, 4099: 1.0, 4200: 1.0, 4213: 1.0, 4276: 3.0, 4428: 1.0, 4527: 2.0, 4564: 4.0, 4577: 1.0, 4638: 4.0, 4645: 1.0, 4659: 2.0, 4670: 1.0, 4929: 6.0, 5115: 1.0, 5158: 1.0, 5293: 1.0, 5691: 1.0, 5861: 1.0, 5884: 1.0, 6054: 1.0, 6094: 3.0, 6638: 1.0, 6844: 1.0, 7188: 1.0, 7283: 1.0, 7349: 1.0, 7392: 1.0, 7948: 2.0, 8027: 1.0, 8201: 1.0, 8774: 1.0, 8843: 1.0, 8918: 1.0, 8936: 1.0, 9282: 1.0, 9325: 1.0, 9511: 1.0, 10002: 1.0, 10237: 1.0, 10606: 1.0, 11081: 1.0, 11587: 3.0, 11680: 1.0, 11683: 2.0, 11974: 1.0, 12216: 2.0, 12723: 1.0, 13143: 1.0, 13620: 1.0, 14270: 3.0, 15482: 1.0, 17052: 1.0, 17537: 1.0, 18095: 2.0, 18537: 1.0, 19096: 1.0, 19945: 1.0, 20545: 1.0, 21722: 1.0, 21989: 1.0, 22049: 1.0, 22793: 1.0, 23081: 1.0, 23951: 1.0, 24478: 1.0, 26078: 1.0, 26207: 1.0, 31347: 4.0, 34822: 2.0, 38049: 1.0, 39480: 1.0, 40750: 1.0, 43025: 1.0, 43822: 4.0, 45691: 1.0, 47126: 1.0, 49579: 2.0, 50997: 1.0, 55731: 5.0, 59949: 1.0, 62510: 1.0, 72886: 1.0, 90752: 1.0, 109100: 1.0, 111011: 1.0, 121775: 2.0, 137872: 1.0, 146825: 1.0, 171862: 1.0, 179565: 1.0, 191449: 1.0, 196125: 1.0, 205509: 1.0, 217605: 1.0, 240246: 1.0, 264393: 1.0, 297133: 1.0, 310887: 1.0, 329358: 1.0, 332271: 1.0, 349515: 1.0})],\n",
       " [5,\n",
       "  SparseVector(370239, {3: 1.0, 7: 1.0, 8: 1.0, 14: 1.0, 27: 1.0, 42: 1.0, 67: 2.0, 76: 1.0, 89: 3.0, 94: 1.0, 98: 1.0, 133: 1.0, 170: 1.0, 178: 1.0, 209: 1.0, 257: 1.0, 260: 2.0, 278: 1.0, 304: 2.0, 565: 1.0, 701: 1.0, 874: 1.0, 1021: 1.0, 1043: 2.0, 1169: 1.0, 1284: 1.0, 1314: 1.0, 1676: 1.0, 2573: 1.0, 3114: 1.0, 3596: 1.0, 3852: 1.0, 5155: 1.0, 10175: 1.0, 11836: 1.0, 111559: 1.0})],\n",
       " [6,\n",
       "  SparseVector(370239, {1: 1.0, 10: 1.0, 14: 1.0, 18: 1.0, 21: 1.0, 36: 1.0, 49: 1.0, 53: 1.0, 57: 1.0, 62: 1.0, 67: 2.0, 72: 1.0, 75: 1.0, 100: 1.0, 108: 1.0, 124: 1.0, 202: 2.0, 223: 1.0, 247: 1.0, 260: 2.0, 278: 1.0, 281: 1.0, 295: 1.0, 302: 1.0, 304: 2.0, 388: 1.0, 440: 1.0, 462: 1.0, 538: 2.0, 690: 1.0, 738: 1.0, 821: 1.0, 897: 1.0, 969: 1.0, 1043: 1.0, 1064: 1.0, 1081: 2.0, 1210: 1.0, 1431: 1.0, 1498: 1.0, 1738: 1.0, 1823: 1.0, 2795: 2.0, 3626: 1.0, 3953: 1.0, 4508: 1.0, 4830: 1.0, 5292: 1.0, 5537: 1.0, 13930: 1.0, 50919: 1.0})],\n",
       " [7,\n",
       "  SparseVector(370239, {14: 2.0, 124: 1.0, 157: 1.0, 306: 2.0, 346: 1.0, 392: 1.0, 514: 1.0, 541: 1.0, 544: 1.0, 697: 1.0, 1080: 1.0, 1508: 1.0, 1812: 1.0, 2331: 1.0, 2367: 1.0, 2516: 1.0, 2975: 1.0, 4488: 1.0, 5203: 1.0, 6841: 1.0})],\n",
       " [8,\n",
       "  SparseVector(370239, {7: 1.0, 28: 1.0, 34: 1.0, 37: 1.0, 44: 1.0, 69: 1.0, 105: 1.0, 118: 1.0, 164: 1.0, 178: 1.0, 239: 1.0, 242: 1.0, 257: 2.0, 260: 6.0, 278: 6.0, 304: 6.0, 350: 1.0, 436: 1.0, 448: 1.0, 554: 1.0, 697: 1.0, 824: 1.0, 1046: 1.0, 1139: 1.0, 1157: 1.0, 1521: 1.0, 1730: 1.0, 1846: 1.0, 1879: 1.0, 2005: 1.0, 2533: 2.0, 2706: 1.0, 3001: 1.0, 3070: 2.0, 4004: 1.0, 4380: 1.0, 5638: 2.0, 11559: 1.0, 12186: 1.0, 27954: 1.0, 29480: 1.0, 47586: 1.0})],\n",
       " [9,\n",
       "  SparseVector(370239, {8: 1.0, 45: 1.0, 46: 1.0, 53: 1.0, 273: 1.0, 346: 1.0, 482: 1.0, 958: 1.0, 972: 1.0, 1253: 1.0})]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('改到不', 194994)\n"
     ]
    }
   ],
   "source": [
    "for a in vocabulary.items():\n",
    "    if a[1]==194994:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o291.trainLDAModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 854.0 failed 1 times, most recent failure: Lost task 5.0 in stage 854.0 (TID 470, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1088)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1082)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.computeGlobalTopicTotals(LDAOptimizer.scala:229)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:171)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:80)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:329)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLDAModel(PythonMLLibAPI.scala:552)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ac4dcbd6cc49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtopic_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribeTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxTermsPerTopic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words_per_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/clustering.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, rdd, k, maxIterations, docConcentration, topicConcentration, seed, checkpointInterval, optimizer)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         model = callMLlibFunc(\"trainLDAModel\", rdd, k, maxIterations,\n\u001b[1;32m   1038\u001b[0m                               \u001b[0mdocConcentration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicConcentration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                               checkpointInterval, optimizer)\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLDAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o291.trainLDAModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 854.0 failed 1 times, most recent failure: Lost task 5.0 in stage 854.0 (TID 470, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1088)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1082)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.computeGlobalTopicTotals(LDAOptimizer.scala:229)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:171)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:80)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:329)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLDAModel(PythonMLLibAPI.scala:552)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 47672)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 313, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 341, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 354, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 681, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 557, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/phejimlin/Documents/dcard_spark/output.txt\", 'w') as f:\n",
    "    lda_model = LDA.train(documents, k=num_topics)\n",
    "\n",
    "    topic_indices = lda_model.describeTopics(maxTermsPerTopic=num_words_per_topic)\n",
    "        \n",
    "    # Print topics, showing the top-weighted 10 terms for each topic\n",
    "    for i in range(len(topic_indices)):\n",
    "        f.write(\"Topic #{0}\\n\".format(i + 1))\n",
    "        for j in range(len(topic_indices[i][0])):\n",
    "            f.write(\"{0}\\t{1}\\n\".format(inv_voc[topic_indices[i][0][j]].encode('utf-8'), topic_indices[i][1][j]))\n",
    "            \n",
    "\n",
    "    f.write(\"{0} topics distributed over {1} documents and {2} unique words\\n\".format(topic_val, documents.count(), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
