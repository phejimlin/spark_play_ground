{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1035, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\", line 883, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\", line 1040, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "java does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f7421fce4582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.extraJavaOptions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dcard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36msetSystemProperty\u001b[0;34m(cls, key, value)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \"\"\"\n\u001b[1;32m    323\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1530\u001b[0m                 answer[proto.CLASS_FQN_START:], self._gateway_client)\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} does not exist in the JVM\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: java does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '16g')\n",
    "SparkContext.setSystemProperty('spark.executor.extraJavaOptions', '-verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps')\n",
    "sc = SparkContext(\"local[*]\", 'dcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "my_spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dcard\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://192.168.2.12:27017/dcard.talk_posts\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://192.168.2.12:27017/dcard.talk_posts\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: integer (nullable = true)\n",
      " |-- anonymousDepartment: boolean (nullable = true)\n",
      " |-- anonymousSchool: boolean (nullable = true)\n",
      " |-- commentCount: integer (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- createdAt: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- excerpt: string (nullable = true)\n",
      " |-- forumAlias: string (nullable = true)\n",
      " |-- forumId: string (nullable = true)\n",
      " |-- forumName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hidden: boolean (nullable = true)\n",
      " |-- hiddenByAuthor: boolean (nullable = true)\n",
      " |-- likeCount: integer (nullable = true)\n",
      " |-- media: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- pinned: boolean (nullable = true)\n",
      " |-- replyId: integer (nullable = true)\n",
      " |-- replyTitle: string (nullable = true)\n",
      " |-- reportReason: string (nullable = true)\n",
      " |-- school: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- updatedAt: string (nullable = true)\n",
      " |-- withNickname: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|     school|count|\n",
      "+-----------+-----+\n",
      "|       æ–°å ¡å¤§å­¸|    9|\n",
      "| åœ‹ç«‹é«˜é›„æµ·æ´‹ç§‘æŠ€å¤§å­¸|   42|\n",
      "|        å±ç§‘å¤§|  446|\n",
      "|      äºç•¶æ£®å¤§å­¸|    1|\n",
      "|       æ ¼é­¯æ–¯ç‰¹|    1|\n",
      "| åœ‹ç«‹è‡ºå—è­·ç†å°ˆç§‘å­¸æ ¡|   22|\n",
      "|     é¦™æ¸¯æµ¸æœƒå¤§å­¸|    1|\n",
      "|    loveâ™çš„â™|    1|\n",
      "|         ä¸ƒä¸ƒ|    1|\n",
      "|   Ladycaca|    1|\n",
      "|       ä¼Šæ¯”çˆ¾å–¬|    1|\n",
      "|      ç†è«–å‹æƒ…è–|    1|\n",
      "|        OwO|    1|\n",
      "|ğŸ’¸æŒ–å£•æºçš„å¥³å­å˜ŸğŸ”¨|    1|\n",
      "|         ç±³ç±³|    1|\n",
      "|     142å°å¥³ç´™|    2|\n",
      "|        ç´…å¿ƒK|    1|\n",
      "|       æ–°é®®ãªè‚|    1|\n",
      "|       ç±³è˜­å¤§å­¸|    2|\n",
      "|  å€«æ•¦å¤§å­¸ä¼¯è²å…‹å­¸é™¢|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"school\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id=6150, anonymousDepartment=False, anonymousSchool=False, commentCount=19, content='å¥½é…·å™¢å¯ä»¥æŒ‰è®šXD', createdAt='2014-04-10T08:16:24.673Z', department='è³‡è¨Šå·¥ç¨‹å­¸ç³»', excerpt='å¥½é…·å™¢å¯ä»¥æŒ‰è®šXD', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='é–’èŠ', gender='M', hidden=False, hiddenByAuthor=False, likeCount=82, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='æ·¡æ±Ÿå¤§å­¸', tags=[], title='æ–°åŠŸèƒ½è€¶ï¼', updatedAt='2014-04-10T08:16:24.673Z', withNickname=None),\n",
       " Row(_id=6151, anonymousDepartment=False, anonymousSchool=False, commentCount=60, content='å¤§å®¶å¿«ä¾†çµ¦é»å»ºè­°å§ : )', createdAt='2014-04-10T09:15:11.945Z', department=' ', excerpt='å¤§å®¶å¿«ä¾†çµ¦é»å»ºè­°å§ : )', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='é–’èŠ', gender='D', hidden=False, hiddenByAuthor=False, likeCount=18, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='ç‹„å¡', tags=[], title='æ–°ç‰ˆå»ºè­°', updatedAt='2014-04-10T09:15:11.945Z', withNickname=None),\n",
       " Row(_id=6152, anonymousDepartment=False, anonymousSchool=False, commentCount=6, content='æˆ–ç”¨æ’°å¯«æ–°æ–‡ç« å›æ‡‰æ‰“æˆé•·é•·ä¸€ç¯‡ï¼', createdAt='2014-04-10T09:40:02.307Z', department='è³‡è¨Šç®¡ç†å­¸ç³»', excerpt='æˆ–ç”¨æ’°å¯«æ–°æ–‡ç« å›æ‡‰æ‰“æˆé•·é•·ä¸€ç¯‡ï¼', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='é–’èŠ', gender='M', hidden=False, hiddenByAuthor=False, likeCount=5, media=[], pinned=False, replyId=6151, replyTitle='æ–°ç‰ˆå»ºè­°', reportReason='', school='åœ‹ç«‹è‡ºç£å¤§å­¸', tags=[], title='Re: æ–°ç‰ˆå»ºè­°', updatedAt='2014-04-10T09:40:02.307Z', withNickname=None),\n",
       " Row(_id=6171, anonymousDepartment=False, anonymousSchool=False, commentCount=20, content='å€‹äººè¦ºå¾—å–œæ­¡çš„æŒ‰éˆ•åŠŸèƒ½å¾ˆæƒ¹äººç…©\\nä¸éœ€è¦ä¸Šå€‹ç¶²å…¨ä¸–ç•Œéƒ½è¦æŒ‰è®šå§QAQ\\nä¸èªç‚ºå–œæ­¡éˆ•çš„æ–°å¢æ˜¯å€‹å¥½ä¸»æ„...', createdAt='2014-04-12T13:36:16.320Z', department='åœ°çƒç§‘å­¸ç³»', excerpt='å€‹äººè¦ºå¾—å–œæ­¡çš„æŒ‰éˆ•åŠŸèƒ½å¾ˆæƒ¹äººç…©\\nä¸éœ€è¦ä¸Šå€‹ç¶²å…¨ä¸–ç•Œéƒ½è¦æŒ‰è®šå§QAQ\\nä¸èªç‚ºå–œæ­¡éˆ•çš„æ–°å¢æ˜¯å€‹å¥½ä¸»æ„...', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='é–’èŠ', gender='M', hidden=False, hiddenByAuthor=False, likeCount=27, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='ä¸­å¤®å¤§å­¸', tags=[], title='æ‹œè¨—ä¸è¦è¶Šä¾†è¶Šåƒè‡‰æ›¸...', updatedAt='2014-04-12T13:36:16.320Z', withNickname=None),\n",
       " Row(_id=6181, anonymousDepartment=False, anonymousSchool=False, commentCount=4, content='https://www.youtube.com/watch?v=JAjXKnFoaZI\\n\\n2014è‡ºå¤§ç•¢æ¥­å­£ä¸»é¡Œæ›²ã€Œé‚„è¨˜å¾—å—ã€\\n\\nè©ï¼šCrispyè„†æ¨‚åœ˜\\næ›²ï¼šSkippy\\n\\né€™æ¢è·¯æˆ‘èµ°äº†å››å¹´ ç¸½ä»°æœ›ç›¡é ­çš„è—å¤©\\nå…©æ’çš„æ¤°å­æ¨¹å°±åƒä¾è¡› å®ˆè­·è‘—æˆ‘å€‘çš„å¾å‰\\nç¸½æœ‰ä¸€å¤©æˆ‘å€‘è¦èªªå†è¦‹ èµ°å…¥å¤§äººçš„ä¸–ç•Œ\\nå€’å½±åœ¨æ¹–é¢ä¸Šçš„æ™‚é–“ åˆæ‚„æ‚„èµ°äº†å¹¾åœˆ\\n\\né‚„è¨˜å¾—å— å°çˆ¸åª½èªªæˆ‘æœƒæ‹¿æ›¸å·ç\\nåœ¨åœ°ä¸‹å®¤ç­‰è‘—è¿æ¥æ›™å…‰\\nçœ‹è¦‹ä½ çš„è¡¨æƒ…å’Œæˆ‘ä¸€æ¨£\\næˆ‘ç¬‘äº†å§\\n\\né‚„è¨˜å¾—å— é¡å­è£¡æ•´é½ŠåŠƒä¸€çš„æ¨¡æ¨£\\nä¸€å€‹äººèº²åœ¨æˆ¿é–“ç·´å‰ä»–\\næ‹¼äº†å‘½è·Ÿä¸Šå¤§å®¶çš„æ­¥ä¼\\næˆ‘å¤§è²å”±\\n\"æ™‚é–“ä¸èƒ½å¤ é‡ä¾† æˆ‘ä¸è¦ç•™ä¸‹ä¸€åˆ†ä¸€ç§’çš„ç©ºç™½\"\\n\\nä½ å°æˆ‘èªªé¢å°æ–°çš„ç”Ÿæ´» æœ‰äº›ä¸å®‰å’Œæƒ¶æ\\næ˜¯æœƒä¸€é£›æ²–å¤©è®Šæˆè‹±é›„ é‚„æ˜¯æµæ˜Ÿèˆ¬å¢œè½\\nåˆ¥å®³æ€• ç›¸ä¿¡è‡ªå·± å’Œé‚£äº›å€”å¼·åŸ·è‘—\\næ¯å€‹äººéƒ½æ˜¯ä¸€ç‰‡å¤©ç©º ç¶»æ”¾è‡ªå·±çš„ç…™ç«\\n\\né‚„è¨˜å¾—å— ä¸€èµ·ç¿¹èª²å é ˜ç±ƒçƒå ´\\nåœ¨å®¿èˆé›»è…¦è£¡é‡‹æ”¾é­”æ³•\\nåœ¨å“ªè£¡è·Œå€’å“ªè£¡å—äº†å‚·\\næˆ‘ä¸æœƒæ€•\\n\\né‚„è¨˜å¾—å— ä¸€èµ·èµ°åœ¨åœ–æ›¸é¤¨çš„é•·å»Š\\nåœ¨è€³é‚Šå°å°è²èªªè‘—å¤¢æƒ³\\nç©ºç„¡ä¸€äººçš„æ¤°æ—å¤§é“ä¸Š\\næˆ‘å¤§è²å”±\\n\"æƒ³è¦æ”¹è®Šå…¨ä¸–ç•Œ é€™è£¡å°±æ˜¯æˆ‘å€‘çš„ èµ·é»\"\\n\\né‚„è¨˜å¾—å— ç¬¬ä¸€æ¬¡é‡è¦‹å¥¹çš„åœ°æ–¹\\nç‰½è‘—æ‰‹æ•£æ­¥åœ¨èˆŸå±±è·¯ä¸Š\\nä¸å†å»åœ¨æ„åˆ¥äººçš„çœ¼å…‰\\næˆ‘å»äº†å¥¹\\n\\né‚„è¨˜å¾—å— å°é¡å­å‘Šè¨´è‡ªå·±è¦å …å¼·\\nä¸ç®¡æœªä¾†æœƒåœ¨å“ªå€‹åœ°æ–¹\\nåœ¨äººæ“ äººçš„ç•¢æ¥­èˆæœƒä¸Š\\næ›ä¸Šè¥¿è£\\n\"äººç”Ÿä¸éœ€è¦100åˆ† èƒ½å’Œä½ å€‘ä¸€èµ·èµ°éå°±å€¼å¾—\"', createdAt='2014-04-13T03:30:48.722Z', department=' ', excerpt='https://www.youtube.com/watch?v=JAjXKnFoaZI\\n\\n2014è‡ºå¤§ç•¢æ¥­å­£ä¸»é¡Œæ›²ã€Œé‚„è¨˜å¾—å—ã€\\n\\nè©ï¼šCrispyè„†æ¨‚åœ˜\\næ›²ï¼šSkippy\\n\\né€™æ¢è·¯æˆ‘èµ°äº†å››å¹´ ç¸½ä»°æœ›ç›¡', forumAlias='talk', forumId='255fd275-fec2-49d2-8e46-2e1557ffaeb0', forumName='é–’èŠ', gender='D', hidden=False, hiddenByAuthor=False, likeCount=26, media=[], pinned=False, replyId=None, replyTitle='null', reportReason='', school='ç‹„å¡', tags=[], title='2014è‡ºå¤§ç•¢æ¥­å­£ä¸»é¡Œæ›²ã€Œé‚„è¨˜å¾—å—ã€MV', updatedAt='2014-04-13T03:30:48.722Z', withNickname=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "content = df.select('content')\n",
    "print(content.rdd.getNumPartitions())\n",
    "content_rdd = content.rdd.repartition(16).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81525\n"
     ]
    }
   ],
   "source": [
    "lineLengths = content_rdd.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_url_and_punctuation(sentence):\n",
    "    # remove url\n",
    "    if 'http' in sentence:\n",
    "        sentence = re.sub(r'[a-zA-Z0-9_/:.]', '', sentence, flags=re.MULTILINE)\n",
    "\n",
    "    # remove punctuation\n",
    "    text_list = re.split('\\W+', sentence)\n",
    "    return list(filter(None, text_list))\n",
    "\n",
    "def to_ngrams(unigrams, length):\n",
    "    return Counter(zip(*[unigrams[i:] for i in range(length)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = 'ğŸ¤”å‰›å‰›è¦å›æ–‡æ™‚å·²ç¶“æ‰¾ä¸åˆ°äº†\\nç¸½ä¹‹ï¼Œå…§å®¹å°±æ˜¯è·Ÿå§Šå§Šä¸€èµ·åˆ®å¡ç„¶å¾Œä¸­äº†205è¬ï¼Œæ–¼æ˜¯ç™¼ä¸Šä¾†çš„ç…§ç‰‡æˆ‘æ€éº¼çœ‹éƒ½æ²’ä¸­ğŸ˜‚ã€‚\\n\\nç¥å¤§å®¶æ–°å¹´å¿«æ¨‚ï¼Œä¹Ÿå¸Œæœ›å¤§å®¶åœ¨åˆ®å½©å·ä¹‹é¡çš„æ±è¥¿èƒ½å¤ å¥½å¥½çœ‹æ¸…æ¥šå–”ğŸ™Œï¼Œä»¥å…é‡åˆ°ä»¥ç‚ºä¸­çä¸€å®¶äººå¾ˆhighï¼Œçµæœè¢«ç¶²å‹èªªæ²’ä¸­çš„å“­å“­æˆ²ç¢¼ã€‚\\nhttps://i.imgur.com/Svbg4BF.jpg\\næ²’æœ‰æˆªæ–‡ç«  æˆªåˆ°åœ–ç‰‡è€Œå·²\\n\\nå¤§å®¶åŠ æ²¹å–”ğŸ‘Š'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['å‰›å‰›è¦å›æ–‡æ™‚å·²ç¶“æ‰¾ä¸åˆ°äº†',\n",
       " 'ç¸½ä¹‹',\n",
       " 'å…§å®¹å°±æ˜¯è·Ÿå§Šå§Šä¸€èµ·åˆ®å¡ç„¶å¾Œä¸­äº†è¬',\n",
       " 'æ–¼æ˜¯ç™¼ä¸Šä¾†çš„ç…§ç‰‡æˆ‘æ€éº¼çœ‹éƒ½æ²’ä¸­',\n",
       " 'ç¥å¤§å®¶æ–°å¹´å¿«æ¨‚',\n",
       " 'ä¹Ÿå¸Œæœ›å¤§å®¶åœ¨åˆ®å½©å·ä¹‹é¡çš„æ±è¥¿èƒ½å¤ å¥½å¥½çœ‹æ¸…æ¥šå–”',\n",
       " 'ä»¥å…é‡åˆ°ä»¥ç‚ºä¸­çä¸€å®¶äººå¾ˆ',\n",
       " 'çµæœè¢«ç¶²å‹èªªæ²’ä¸­çš„å“­å“­æˆ²ç¢¼',\n",
       " 'æ²’æœ‰æˆªæ–‡ç« ',\n",
       " 'æˆªåˆ°åœ–ç‰‡è€Œå·²',\n",
       " 'å¤§å®¶åŠ æ²¹å–”']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url_and_punctuation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result = content_rdd.map(lambda a: remove_url_and_punctuation(a['content'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.78852820396423\n"
     ]
    }
   ],
   "source": [
    "# compute unigram and bigram count\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "result = content_rdd.map(lambda a: remove_url_and_punctuation(a['content'])).collect()\n",
    "\n",
    "for post in result:\n",
    "    for line in post:\n",
    "        unigram_counter.update(line)\n",
    "        bigram_counter.update(to_ngrams(line, 2))\n",
    "        trigram_counter.update(to_ngrams(line, 3))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('çš„', 555164), ('æˆ‘', 355176), ('æ˜¯', 336489), ('ä¸', 258415), ('ä¸€', 244291), ('æœ‰', 243244), ('äº†', 175684), ('äºº', 147514), ('åœ¨', 140303), ('åˆ°', 127492)]\n",
      "[(('æ²’', 'æœ‰'), 44669), (('ä»€', 'éº¼'), 42759), (('å¯', 'ä»¥'), 40630), (('ä¸€', 'å€‹'), 38825), (('è‡ª', 'å·±'), 38158), (('çŸ¥', 'é“'), 37187), (('å¤§', 'å®¶'), 36745), (('è¦º', 'å¾—'), 35109), (('å› ', 'ç‚º'), 34636), (('çœŸ', 'çš„'), 33224)]\n",
      "[(('ä¸', 'çŸ¥', 'é“'), 18890), (('çš„', 'æ™‚', 'å€™'), 15379), (('æœ‰', 'æ²’', 'æœ‰'), 13914), (('ç‚º', 'ä»€', 'éº¼'), 9675), (('è‡ª', 'å·±', 'çš„'), 8395), (('_', '_', '_'), 7672), (('å“ˆ', 'å“ˆ', 'å“ˆ'), 7248), (('æˆ‘', 'è¦º', 'å¾—'), 6490), (('æ˜¯', 'ä¸', 'æ˜¯'), 6301), (('çœŸ', 'çš„', 'å¾ˆ'), 6168)]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_counter.most_common(10))\n",
    "print(bigram_counter.most_common(10))\n",
    "print(trigram_counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_to_three_grams(line):\n",
    "    print(line)\n",
    "    result = (Counter(line), to_ngrams(line, 2), to_ngrams(line, 3))\n",
    "    print(len(result[0]), len(result[1]), len(result[2]))\n",
    "    return result\n",
    "#     return (Counter(line), to_ngrams(line, 2), to_ngrams(line, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(content='ğŸ¤”ğŸ¤”\\nè½èªªä»Šå¹´åœ¨å±æ±æŸåœ°çš„æ½®Xé«˜ä¸­\\nå…¨åœ‹ç¹æ˜Ÿç¬¬ä¸€ (110äºº)\\nä½†åªæœ‰46å€‹äººä¸Šåœ‹ç«‹\\né›£é“é€™å°±æ˜¯æ‰€è¬‚æœ‰å­¸æ ¡å°±è®€çš„æ¦‚å¿µå—?\\n\\né‚„æœ‰æ“šèªªç¹æ˜Ÿé€²å¤§å­¸çš„ éƒ½è »å„ªç§€çš„\\næ˜¯é€™æ¨£å—ï¼Ÿ')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['è½èªªä»Šå¹´åœ¨å±æ±æŸåœ°çš„æ½®Xé«˜ä¸­',\n",
       " 'å…¨åœ‹ç¹æ˜Ÿç¬¬ä¸€',\n",
       " '110äºº',\n",
       " 'ä½†åªæœ‰46å€‹äººä¸Šåœ‹ç«‹',\n",
       " 'é›£é“é€™å°±æ˜¯æ‰€è¬‚æœ‰å­¸æ ¡å°±è®€çš„æ¦‚å¿µå—',\n",
       " 'é‚„æœ‰æ“šèªªç¹æ˜Ÿé€²å¤§å­¸çš„',\n",
       " 'éƒ½è »å„ªç§€çš„',\n",
       " 'æ˜¯é€™æ¨£å—']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(content_rdd.top(1))\n",
    "remove_url_and_punctuation(content_rdd.top(1)[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15678028\n"
     ]
    }
   ],
   "source": [
    "lineLengths = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# sec = int(round(time.time()))\n",
    "# print(sec)\n",
    "# sample_rdd = content_rdd.sample(False, 0.01, sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda line: one_to_three_grams(line)).reduce(lambda a, b: tuple(map(operator.add, a, b)))\n",
    "\n",
    "# result[0].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# result_one_grams = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda s: Counter(s)).reduce(lambda a,b: a + b)\n",
    "\n",
    "# result_one_grams.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Utilizing spark Accumulator to calculate n-grams.\n",
    "# list_data = content_rdd.flatMap(lambda a: remove_url(a['content'])).collect()\n",
    "# result_list = sc.parallelize(list_data)\n",
    "# result_list.top(10)\n",
    "\n",
    "# class CounterAccumulatorParam(AccumulatorParam):\n",
    "#     def zero(self, initialValue):\n",
    "#         return initialValue\n",
    "\n",
    "#     def addInPlace(self, v1, v2):\n",
    "#         v1 += v2\n",
    "#         return v1\n",
    "\n",
    "# # Then, create an Accumulator of this type:\n",
    "# one_gram_accum = sc.accumulator(Counter(), CounterAccumulatorParam())\n",
    "# two_gram_accum = sc.accumulator(Counter(), CounterAccumulatorParam())\n",
    "# three_gram_accum = sc.accumulator(Counter(), CounterAccumulatorParam())\n",
    "\n",
    "# def one_to_three_grams_accum(line):\n",
    "#     one_gram_accum.add(Counter(line))\n",
    "#     two_gram_accum.add(to_ngrams(line, 2))\n",
    "#     three_gram_accum.add(to_ngrams(line, 3))\n",
    "\n",
    "# result_list.foreach(lambda line: one_to_three_grams_accum(line))\n",
    "\n",
    "# one_gram_accum.value.most_common(10)\n",
    "\n",
    "# two_gram_accum.value.most_common(10)\n",
    "\n",
    "# three_gram_accum.value.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " # Good-Turing Smoothing Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "V1 = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).map(lambda s: len(s)).reduce(lambda a, b: a + b)\n",
    "V2 = V1 ** 2\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15670476 245800561178671\n"
     ]
    }
   ],
   "source": [
    "from math import log10\n",
    "# compute N1, N2, N3...\n",
    "unigram_Nr = Counter(unigram_counter.values())\n",
    "bigram_Nr = Counter(bigram_counter.values())\n",
    "# compute N0\n",
    "unigram_Nr[0] = V1 - len(unigram_counter)\n",
    "bigram_Nr[0] = V2 - len(bigram_counter)\n",
    "print(unigram_Nr[0], bigram_Nr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.942992669782335e-05, 1.0183823529411764, 1.6570397111913358, 3.281045751633987, 3.545816733067729, 5.52808988764045, 5.378048780487805, 7.428571428571429, 8.461538461538462, 9.272727272727273]\n",
      "[1.48258408464375e-09, 0.6455353712749026, 1.5699225491613036, 2.507757542280636, 3.497926924073594, 4.4898322035781755, 5.4324324324324325, 6.407449929837989, 7.322847813968305, 8.6522462562396]\n"
     ]
    }
   ],
   "source": [
    "# compute r\n",
    "unigram_r = [(i+1) * unigram_Nr[i+1] / unigram_Nr[i] for i in range(k)]\n",
    "bigram_r = [(i+1) * bigram_Nr[i+1] / bigram_Nr[i] for i in range(k)]\n",
    "print(unigram_r)\n",
    "print(bigram_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15678028 13901455\n"
     ]
    }
   ],
   "source": [
    "# compute normalize factor\n",
    "# compute N\n",
    "unigram_N = sum(unigram_counter.values())\n",
    "bigram_N = sum(bigram_counter.values())\n",
    "print(unigram_N, bigram_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15679048 13989855\n"
     ]
    }
   ],
   "source": [
    "# compute new probability sum\n",
    "unigram_N_ = unigram_N + k * unigram_Nr[k]\n",
    "bigram_N_ = bigram_N + k * bigram_Nr[k]\n",
    "print(unigram_N_, bigram_N_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999349450298258 0.9936811353655917\n"
     ]
    }
   ],
   "source": [
    "# normalize factor: N/Nâ€™\n",
    "unigram_norm_factor = unigram_N / unigram_N_\n",
    "bigram_norm_factor = bigram_N / bigram_N_\n",
    "print(unigram_norm_factor, bigram_norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Estimating P(w) and P(wâ€™|w)\n",
    "def prob_1word(unigram):\n",
    "    count = unigram_counter[unigram]\n",
    "    r = unigram_r[count] if count < k else count\n",
    "    return log10(r / unigram_N_)\n",
    "def prob_2words(text_front, text_rear):\n",
    "    count = bigram_counter[text_front, text_rear]\n",
    "    r = bigram_r[count] if count < k else count\n",
    "    return log10(r / bigram_N_)\n",
    "def prob_word_by_word(text_front, text_rear):\n",
    "    return prob_2words(text_front, text_rear) - prob_1word(text_front)\n",
    "def prob_words(words):\n",
    "    return prob_1word(words[0]) + sum(prob_word_by_word(words[i-1], words[i]) for i in range(1, len(words)))\n",
    "def prob_text(text):\n",
    "    return prob_words(text.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.352024606932168\n",
      "-5.231999360819604\n",
      "-1.8624478784734173\n"
     ]
    }
   ],
   "source": [
    "print(prob_1word('æ¸…'))\n",
    "print(prob_2words('æ¸…','è¯'))\n",
    "print(prob_word_by_word('æˆ‘','å¾ˆ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unicount_log = {k: log10(v) for k, v in unigram_counter.items()}\n",
    "bicount_log = {k: log10(v) for k, v in bigram_counter.items()}\n",
    "unigram_r_log = [log10(r) for r in unigram_r]\n",
    "bigram_r_log = [log10(r) for r in bigram_r]\n",
    "unigram_N_log = log10(unigram_N_)\n",
    "bigram_N_log = log10(bigram_N_)\n",
    "\n",
    "\n",
    "def prob_1word(unigram):\n",
    "    count = unigram_counter[unigram]\n",
    "    r = unigram_r_log[count] if count < k else unicount_log[unigram]\n",
    "    return r - unigram_N_log\n",
    "def prob_2words(text_front, text_rear):\n",
    "    count = bigram_counter[text_front, text_rear]\n",
    "    r = bigram_r_log[count] if count < k else bicount_log[text_front, text_rear]\n",
    "    return r - bigram_N_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.3520246069321686\n",
      "-3.7596345517270455\n",
      "-5.231999360819604\n",
      "-1.879974753887435\n"
     ]
    }
   ],
   "source": [
    "print(prob_1word(u'æ¸…'))\n",
    "print(prob_1word(u'è¯'))\n",
    "print(prob_2words(u'æ¸…', u'è¯'))\n",
    "print(prob_word_by_word(u'æ¸…',u'è¯'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# N_unigram_corpus = math.log2(float(sum(unigram_counter.values())))\n",
    "# N_bigram_corpus = math.log2(float(sum(bigram_counter.values())))\n",
    "def pmi(words):\n",
    "    word1 = words[0]\n",
    "    word2 = words[1]\n",
    "    # Good-Turing Estimation å°‡æ¬¡æ•¸å°æ–¼kçš„å­—åšä¸€äº›èª¿æ•´ ç›®æ¨™ä¸è¦è®“æ²’å‡ºç¾éçš„å­— æ©Ÿç‡ç‚º0\n",
    "    count_word1 = unigram_counter[word1]\n",
    "    r_word1 = unigram_r_log[count_word1] if count_word1 < k else unicount_log[word1]\n",
    "    count_word2 = unigram_counter[word2]\n",
    "    r_word2 = unigram_r_log[count_word2] if count_word2 < k else unicount_log[word2]\n",
    "    count_word1_and_word2 = bigram_counter[(word1, word2)]\n",
    "    r_word1_and_word2 = bigram_r_log[count_word1_and_word2] if count_word1_and_word2 < k else bicount_log[(word1, word2)]\n",
    "\n",
    "    # mutual information algorithm\n",
    "    prob_word1 = r_word1 - unigram_N_log\n",
    "    prob_word2 = r_word2 -  unigram_N_log\n",
    "    prob_word1_word2 = r_word1_and_word2 -  bigram_N_log\n",
    "    return prob_word1_word2 - (prob_word1+prob_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.282093830811121\n",
      "0.6955342288888389\n",
      "1.5351483023990387\n",
      "2.5625752923560103\n",
      "0.632807350998914\n"
     ]
    }
   ],
   "source": [
    "print(pmi((u'è°', u'æ€')))\n",
    "print(pmi((u'å¾ˆ',u'é–‹')))\n",
    "print(pmi((u'é–‹', u'å¿ƒ')))\n",
    "print(pmi((u'åƒ', u'é£¯')))\n",
    "print(pmi((u'æˆ‘', u'å¼Ÿ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "threshold = 0.5\n",
    "\n",
    "def word_segmentation(sentence):\n",
    "    # input : å¿˜äº†ç•¶åˆé¸çš„é¸é …  \n",
    "    \n",
    "    # Handle empty sentence.\n",
    "    if len(sentence)==0:\n",
    "        return sentence\n",
    "    words_list = to_words(sentence, 2)\n",
    "    max_probability_dict = find_max_prob(to_prob_dict(words_list))\n",
    "    sentences = seperate_sentence(sentence, max_probability_dict)\n",
    "    return sentences.split()\n",
    "\n",
    "def to_words(unigrams, length):\n",
    "    return list(zip(*[unigrams[i:] for i in range(length)]))\n",
    "\n",
    "\n",
    "def to_prob_dict(words_list):\n",
    "#   input : [('å¿˜', 'äº†'), ('äº†', 'ç•¶'), ('ç•¶', 'åˆ'), ('åˆ', 'é¸'), ('é¸', 'çš„'), ('çš„', 'é¸'), ('é¸', 'é …')]\n",
    "    result_sentence = {}\n",
    "    for word in words_list:\n",
    "        result_sentence[word] = pmi(word)\n",
    "    return result_sentence\n",
    "\n",
    "\n",
    "def find_max_prob(probability_dict):\n",
    "    sorted_prob = sorted(probability_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#     print(sorted_prob)  # Can see every candidate words prob.\n",
    "    result_list = []\n",
    "    for candidate in sorted_prob:\n",
    "        prob = candidate[1]\n",
    "        words_tuple = candidate[0]\n",
    "        if prob > threshold:\n",
    "            result_list.append(''.join(map(str, words_tuple)))  # ('é¸', 'é …') => é¸é …\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def seperate_sentence(orginal_sentence, max_probability_dict):\n",
    "    segment_word = orginal_sentence\n",
    "    for candidate in max_probability_dict:\n",
    "        insert_word = \" \"+candidate+\" \"\n",
    "        segment_word = segment_word.replace(candidate, insert_word)\n",
    "    return segment_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æˆ‘', 'å·²ç¶“', 'å‘', 'å…­å€‹', 'å¥³ç”Ÿ', 'é‚€è«‹', 'å¥¹å€‘', 'ç•¶æˆ‘', 'èˆä¼´']\n"
     ]
    }
   ],
   "source": [
    "test = word_segmentation(u'æˆ‘å·²ç¶“å‘å…­å€‹å¥³ç”Ÿé‚€è«‹å¥¹å€‘ç•¶æˆ‘èˆä¼´')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_lines = content_rdd.flatMap(lambda s: remove_url_and_punctuation(s['content'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‘ƒ\n",
      "['å‘ƒ']\n",
      "\n",
      "å¿˜äº†ç•¶åˆé¸çš„é¸é …\n",
      "['å¿˜äº†', 'ç•¶åˆ', 'é¸çš„', 'é¸é …']\n",
      "\n",
      "æƒ³è¦çŸ¥é“çš„è©±å¯ä»¥é¡¯ç¤ºå—\n",
      "['æƒ³è¦', 'çŸ¥é“', 'çš„è©±', 'å¯ä»¥', 'é¡¯ç¤º', 'å—']\n",
      "\n",
      "æˆ‘çŒœç·šåœ¨æ˜¯ä¸è¡Œ\n",
      "['æˆ‘çŒœ', 'ç·šåœ¨æ˜¯', 'ä¸è¡Œ']\n",
      "\n",
      "è¦æ”¹äº†ç³»çµ±æ‰èƒ½å§\n",
      "['è¦æ”¹', 'äº†', 'ç³»çµ±', 'æ‰èƒ½', 'å§']\n",
      "\n",
      "æœ‰æ™‚å€™é›£å¾—é‡åˆ°èªè­˜çš„äººå…¶å¯¦å¾ˆé«˜èˆˆ\n",
      "['æœ‰', 'æ™‚å€™', 'é›£å¾—', 'é‡åˆ°', 'èªè­˜', 'çš„äºº', 'å…¶å¯¦', 'å¾ˆ', 'é«˜èˆˆ']\n",
      "\n",
      "ä¸éå› ç‚ºå¤ªä¹…æ²’çœ‹åˆ°\n",
      "['ä¸é', 'å› ç‚º', 'å¤ªä¹…', 'æ²’', 'çœ‹åˆ°']\n",
      "\n",
      "ä¸çŸ¥é“è¦èªªä»€éº¼\n",
      "['ä¸', 'çŸ¥é“', 'è¦èªª', 'ä»€éº¼']\n",
      "\n",
      "æ‰€ä»¥å°±å¿«æ­¥é›¢é–‹äº†\n",
      "['æ‰€ä»¥', 'å°±', 'å¿«æ­¥', 'é›¢é–‹', 'äº†']\n",
      "\n",
      "å¤§å®¶éƒ½æœ‰é€™æ¨£çš„ç¶“é©—å—\n",
      "['å¤§å®¶', 'éƒ½æœ‰', 'é€™æ¨£', 'çš„', 'ç¶“é©—', 'å—']\n",
      "\n",
      "æ„Ÿè¦ºæ·¡æ±Ÿçš„åŒå­¸å¥½å¤šé˜¿\n",
      "['æ„Ÿè¦º', 'æ·¡æ±Ÿ', 'çš„', 'åŒå­¸', 'å¥½å¤š', 'é˜¿']\n",
      "\n",
      "æŠ½åˆ°å¥½å¤šéƒ½æ˜¯æ·¡æ±Ÿçš„\n",
      "['æŠ½åˆ°', 'å¥½å¤š', 'éƒ½æ˜¯', 'æ·¡æ±Ÿ', 'çš„']\n",
      "\n",
      "å¥½å‹ä¹Ÿéƒ½æ˜¯\n",
      "['å¥½å‹', 'ä¹Ÿ', 'éƒ½æ˜¯']\n",
      "\n",
      "å­¸æ ¡å³å°‡èˆ‰è¡Œè€¶èª•èˆæœƒ\n",
      "['å­¸æ ¡', 'å³å°‡', 'èˆ‰è¡Œ', 'è€¶èª•', 'èˆæœƒ']\n",
      "\n",
      "æˆ‘å·²ç¶“å‘å…­å€‹å¥³ç”Ÿé‚€è«‹å¥¹å€‘ç•¶æˆ‘èˆä¼´\n",
      "['æˆ‘', 'å·²ç¶“', 'å‘', 'å…­å€‹', 'å¥³ç”Ÿ', 'é‚€è«‹', 'å¥¹å€‘', 'ç•¶æˆ‘', 'èˆä¼´']\n",
      "\n",
      "ä½†æ˜¯éƒ½è¢«æ‹’çµ•\n",
      "['ä½†æ˜¯', 'éƒ½è¢«', 'æ‹’çµ•']\n",
      "\n",
      "æˆ‘å¾ˆé›£é\n",
      "['æˆ‘å¾ˆ', 'é›£é']\n",
      "\n",
      "æˆ‘ç„¡æ³•å†æ‰¿å—ä»»ä½•æ‰“æ“Š\n",
      "['æˆ‘', 'ç„¡æ³•', 'å†', 'æ‰¿å—', 'ä»»ä½•', 'æ‰“æ“Š']\n",
      "\n",
      "æˆ‘å¿ƒç†ç³»åŒå­¸å°æˆ‘èªª\n",
      "['æˆ‘', 'å¿ƒç†', 'ç³»', 'åŒå­¸', 'å°', 'æˆ‘èªª']\n",
      "\n",
      "ä¸è¦é›£é\n",
      "['ä¸è¦', 'é›£é']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in total_lines[:20]:\n",
    "    print(sentence)\n",
    "    print(word_segmentation(sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def partial_match(word, counter):\n",
    "    new_counter = Counter()\n",
    "    for key, value in counter.most_common():\n",
    "        if all(k1 == k2 or k2 is None for k1, k2 in zip(key, word)):\n",
    "            new_counter[key] = value\n",
    "    return new_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "one_match = partial_match((u'æ²’', None), bigram_counter)\n",
    "two_match = partial_match((u'æ²’', u'æœ‰', None), trigram_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('æ²’', 'æœ‰'), 44669),\n",
       " (('æ²’', 'ä»€'), 2285),\n",
       " (('æ²’', 'æƒ³'), 2201),\n",
       " (('æ²’', 'è¾¦'), 2084),\n",
       " (('æ²’', 'äºº'), 1754),\n",
       " (('æ²’', 'çœ‹'), 1645),\n",
       " (('æ²’', 'é—œ'), 1577),\n",
       " (('æ²’', 'äº‹'), 1472),\n",
       " (('æ²’', 'éŒ¯'), 1344),\n",
       " (('æ²’', 'å¤š'), 864)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_match.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('æ²’', 'æœ‰', 'äºº'), 4608),\n",
       " (('æ²’', 'æœ‰', 'ä»€'), 1272),\n",
       " (('æ²’', 'æœ‰', 'æ¨'), 838),\n",
       " (('æ²’', 'æœ‰', 'é€™'), 832),\n",
       " (('æ²’', 'æœ‰', 'å¾ˆ'), 806),\n",
       " (('æ²’', 'æœ‰', 'è¦'), 713),\n",
       " (('æ²’', 'æœ‰', 'ä¸€'), 705),\n",
       " (('æ²’', 'æœ‰', 'æƒ³'), 627),\n",
       " (('æ²’', 'æœ‰', 'ä»»'), 504),\n",
       " (('æ²’', 'æœ‰', 'çœ‹'), 495)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_match.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/data/mllib/sample_lda_data.txt MapPartitionsRDD[37] at textFile at NativeMethodAccessorImpl.java:0\n",
      "PythonRDD[38] at RDD at PythonRDD.scala:48\n",
      "PythonRDD[40] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "# Load and parse the data\n",
    "data = sc.textFile(\"/usr/local/spark/data/mllib/sample_lda_data.txt\")\n",
    "print(data)\n",
    "parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "print(parsedData)\n",
    "# Index documents with unique IDs\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics (as distributions over vocab of 11 words):\n",
      "Topic 0:\n",
      " 9.83124703324\n",
      " 6.26635606566\n",
      " 2.28796463598\n",
      " 10.3650763441\n",
      " 6.11907922619\n",
      " 8.32748840998\n",
      " 14.8422943338\n",
      " 2.51740931974\n",
      " 3.12135581995\n",
      " 9.55781644441\n",
      " 13.0525672405\n",
      "Topic 1:\n",
      " 7.3914021596\n",
      " 9.62750309174\n",
      " 7.80180750758\n",
      " 12.3654287092\n",
      " 7.20838108364\n",
      " 3.59494206471\n",
      " 8.94472034523\n",
      " 1.78870314191\n",
      " 3.34569455641\n",
      " 6.7349073487\n",
      " 17.6358217628\n",
      "Topic 2:\n",
      " 8.77735080716\n",
      " 13.1061408426\n",
      " 1.91022785644\n",
      " 17.2694949467\n",
      " 11.6725396902\n",
      " 10.0775695253\n",
      " 7.21298532093\n",
      " 5.69388753835\n",
      " 1.53294962363\n",
      " 7.70727620689\n",
      " 2.31161099666\n"
     ]
    }
   ],
   "source": [
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3)\n",
    "\n",
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n",
    "      + \" words):\")\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, ldaModel.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "num_of_stop_words = 50      # Number of most common words to remove, trying to eliminate stop words\n",
    "num_topics = 3              # Number of topics we are looking for\n",
    "num_words_per_topic = 10    # Number of words to display for each topic\n",
    "max_iterations = 35         # Max number of times to iterate before finishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('æ²’', 'æœ‰'), 44669),\n",
       " (('ä»€', 'éº¼'), 42759),\n",
       " (('å¯', 'ä»¥'), 40630),\n",
       " (('ä¸€', 'å€‹'), 38825),\n",
       " (('è‡ª', 'å·±'), 38158),\n",
       " (('çŸ¥', 'é“'), 37187),\n",
       " (('å¤§', 'å®¶'), 36745),\n",
       " (('è¦º', 'å¾—'), 35109),\n",
       " (('å› ', 'ç‚º'), 34636),\n",
       " (('çœŸ', 'çš„'), 33224),\n",
       " (('æˆ‘', 'å€‘'), 31995),\n",
       " (('ä¸', 'æ˜¯'), 28781),\n",
       " (('æ‰€', 'ä»¥'), 28237),\n",
       " (('æœ‹', 'å‹'), 27289),\n",
       " (('å°±', 'æ˜¯'), 26864),\n",
       " (('æ™‚', 'å€™'), 26351),\n",
       " (('é‚„', 'æ˜¯'), 26081),\n",
       " (('é€™', 'æ¨£'), 24335),\n",
       " (('ä¸', 'çŸ¥'), 21375),\n",
       " (('ç„¶', 'å¾Œ'), 21356),\n",
       " (('çœ‹', 'åˆ°'), 21188),\n",
       " (('çš„', 'äºº'), 20874),\n",
       " (('æˆ‘', 'çš„'), 20783),\n",
       " (('æ˜¯', 'æˆ‘'), 20324),\n",
       " (('æœ‰', 'äºº'), 19600),\n",
       " (('æ€', 'éº¼'), 19496),\n",
       " (('ç¾', 'åœ¨'), 18760),\n",
       " (('ä½†', 'æ˜¯'), 18669),\n",
       " (('ä»–', 'å€‘'), 18649),\n",
       " (('çš„', 'æ™‚'), 18577),\n",
       " (('å¦‚', 'æœ'), 17887),\n",
       " (('ä¸€', 'ä¸‹'), 17608),\n",
       " (('ä¸', 'æœƒ'), 16947),\n",
       " (('é€™', 'å€‹'), 16427),\n",
       " (('é–‹', 'å§‹'), 16150),\n",
       " (('å–œ', 'æ­¡'), 15823),\n",
       " (('å¾ˆ', 'å¤š'), 15611),\n",
       " (('å“ˆ', 'å“ˆ'), 15334),\n",
       " (('ä¸€', 'ç›´'), 15143),\n",
       " (('éƒ½', 'æ˜¯'), 14362),\n",
       " (('ä¸€', 'æ¨£'), 14308),\n",
       " (('å€‹', 'äºº'), 14118),\n",
       " (('æœ‰', 'æ²’'), 14033),\n",
       " (('ä¸', 'è¦'), 13770),\n",
       " (('æ¯”', 'è¼ƒ'), 13766),\n",
       " (('æœ‰', 'ä¸€'), 13737),\n",
       " (('äº†', 'ä¸€'), 13654),\n",
       " (('æœ€', 'è¿‘'), 13521),\n",
       " (('æ˜¯', 'ä¸'), 13367),\n",
       " (('ä»Š', 'å¤©'), 13314)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sc.parallelize(result)\n",
    "unigram_counter.most_common(50)\n",
    "bigram_counter.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def document_segmentation(document):\n",
    "    sentences = remove_url_and_punctuation(document)  # output ['å‘ƒ', 'å¿˜äº†ç•¶åˆé¸çš„é¸é …', 'æƒ³è¦çŸ¥é“çš„è©±å¯ä»¥é¡¯ç¤ºå—', 'æˆ‘çŒœç·šåœ¨æ˜¯ä¸è¡Œ', 'è¦æ”¹äº†ç³»çµ±æ‰èƒ½å§']\n",
    "    \n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        # Handle empty sentence.\n",
    "        if len(sentence)==0:\n",
    "            return sentence\n",
    "        words_list = to_words(sentence, 2)\n",
    "        max_probability_dict = find_max_prob(to_prob_dict(words_list))\n",
    "        sentences = seperate_sentence(sentence, max_probability_dict)\n",
    "        sentence_list = sentence_list + sentences.split()\n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "# Convert the given document into a vector of word counts\n",
    "def document_vector(document):\n",
    "    id = document[1]\n",
    "    counts = Counter()\n",
    "    for token in document[0]:\n",
    "        if token in vocabulary:\n",
    "            token_id = vocabulary[token]\n",
    "            counts[token_id] += 1\n",
    "    counts = sorted(counts.items())\n",
    "    if len(counts) > 0 :\n",
    "        keys = [x[0] for x in counts]\n",
    "        values = [x[1] for x in counts]\n",
    "    else:\n",
    "        keys = [0]\n",
    "        values = [0]\n",
    "    return (id, Vectors.sparse(len(vocabulary), keys, values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_id=10003, content='å‘ƒ....å¿˜äº†ç•¶åˆé¸çš„é¸é …ï¼Œæƒ³è¦çŸ¥é“çš„è©±å¯ä»¥é¡¯ç¤ºå—?\\r\\næˆ‘çŒœç·šåœ¨æ˜¯ä¸è¡Œ...è¦æ”¹äº†ç³»çµ±æ‰èƒ½å§?\\r\\n')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = df.select(df['_id'], df['content'])\n",
    "content_rdd = content.rdd.repartition(16).cache()\n",
    "# æŠŠåŸæœ¬dcardçš„ idèˆ‡æ¨™é¡Œ åšæ¯”å°\n",
    "\n",
    "\n",
    "content_rdd.take(1)[0]\n",
    "# result_rdd.map(lambda sentence: word_segmentation(sentence)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result_rdd = content_rdd.map(lambda posts: document_segmentation(posts['content'])).filter(lambda documnet: len(documnet)>0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['å‘ƒ',\n",
       "  'å¿˜äº†',\n",
       "  'ç•¶åˆ',\n",
       "  'é¸çš„',\n",
       "  'é¸é …',\n",
       "  'æƒ³è¦',\n",
       "  'çŸ¥é“',\n",
       "  'çš„è©±',\n",
       "  'å¯ä»¥',\n",
       "  'é¡¯ç¤º',\n",
       "  'å—',\n",
       "  'æˆ‘çŒœ',\n",
       "  'ç·šåœ¨æ˜¯',\n",
       "  'ä¸è¡Œ',\n",
       "  'è¦æ”¹',\n",
       "  'äº†',\n",
       "  'ç³»çµ±',\n",
       "  'æ‰èƒ½',\n",
       "  'å§']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentence_counts = result_rdd.flatMap(lambda document: document) \\\n",
    "    .map(lambda word: (word, 1) if word is not list else ('', 0)) \\\n",
    "    .reduceByKey( lambda x,y: x + y) \\\n",
    "    .map(lambda tuple: (tuple[1], tuple[0])) \\\n",
    "    .sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Identify a threshold to remove the top words, in an effort to remove stop words\n",
    "threshold_value = sentence_counts.take(num_of_stop_words)[num_of_stop_words - 1][0]\n",
    "\n",
    "# Only keep words with a count less than the threshold identified above, and then index each one and collect them into a map\n",
    "vocabulary = sentence_counts                    \\\n",
    "    .filter(lambda x : x[0] < threshold_value)  \\\n",
    "    .map(lambda x: x[1])                        \\\n",
    "    .zipWithIndex()                             \\\n",
    "    .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "documents = result_rdd.zipWithIndex().map(document_vector).map(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inv_voc = {value: key for (key, value) in vocabulary.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  SparseVector(370239, {33: 1.0, 92: 1.0, 100: 1.0, 532: 1.0, 559: 1.0, 723: 1.0, 892: 1.0, 1043: 1.0, 1110: 1.0, 1635: 1.0, 2763: 1.0, 4281: 1.0, 8906: 1.0, 12721: 1.0, 188274: 1.0})],\n",
       " [1,\n",
       "  SparseVector(370239, {7: 1.0, 22: 1.0, 56: 1.0, 127: 1.0, 144: 1.0, 192: 1.0, 209: 1.0, 480: 1.0, 2107: 1.0, 2194: 1.0, 2411: 1.0, 2903: 1.0, 18201: 1.0})],\n",
       " [2,\n",
       "  SparseVector(370239, {14: 2.0, 31: 1.0, 35: 1.0, 107: 1.0, 506: 1.0, 687: 2.0, 853: 1.0, 3923: 2.0})],\n",
       " [3,\n",
       "  SparseVector(370239, {27: 1.0, 30: 1.0, 34: 1.0, 35: 1.0, 67: 1.0, 77: 1.0, 127: 1.0, 152: 1.0, 180: 1.0, 189: 1.0, 378: 1.0, 387: 2.0, 503: 2.0, 505: 2.0, 583: 1.0, 663: 2.0, 683: 1.0, 903: 1.0, 953: 1.0, 1052: 1.0, 1065: 1.0, 1241: 1.0, 1400: 1.0, 1745: 1.0, 1851: 2.0, 1877: 1.0, 2142: 1.0, 2498: 1.0, 2565: 1.0, 2896: 1.0, 3404: 1.0, 3929: 1.0, 5423: 1.0, 5460: 1.0, 8728: 1.0, 9388: 1.0, 9487: 1.0, 10895: 2.0, 12669: 1.0, 15978: 1.0, 23230: 1.0, 80429: 1.0, 153157: 1.0})],\n",
       " [4,\n",
       "  SparseVector(370239, {1: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, 17: 1.0, 34: 3.0, 40: 3.0, 63: 1.0, 70: 1.0, 77: 1.0, 94: 1.0, 123: 2.0, 125: 1.0, 146: 1.0, 149: 1.0, 151: 1.0, 161: 1.0, 189: 1.0, 193: 1.0, 201: 1.0, 218: 1.0, 221: 1.0, 227: 2.0, 264: 1.0, 280: 3.0, 316: 1.0, 325: 1.0, 331: 1.0, 387: 1.0, 388: 1.0, 404: 1.0, 462: 1.0, 463: 2.0, 532: 1.0, 550: 2.0, 573: 4.0, 579: 1.0, 598: 1.0, 599: 1.0, 614: 2.0, 615: 2.0, 620: 1.0, 621: 2.0, 639: 1.0, 652: 1.0, 657: 1.0, 666: 1.0, 673: 1.0, 738: 1.0, 825: 1.0, 964: 1.0, 1007: 1.0, 1030: 1.0, 1067: 1.0, 1075: 1.0, 1092: 1.0, 1145: 1.0, 1159: 1.0, 1193: 3.0, 1250: 1.0, 1271: 1.0, 1321: 1.0, 1382: 1.0, 1442: 1.0, 1538: 1.0, 1552: 1.0, 1575: 1.0, 1622: 1.0, 1628: 1.0, 1634: 1.0, 1714: 1.0, 1717: 1.0, 1736: 2.0, 1766: 1.0, 1777: 5.0, 1792: 1.0, 1852: 1.0, 1853: 2.0, 1894: 1.0, 1942: 1.0, 1999: 1.0, 2006: 1.0, 2022: 1.0, 2205: 1.0, 2247: 2.0, 2291: 1.0, 2349: 1.0, 2376: 2.0, 2499: 1.0, 2528: 1.0, 2631: 1.0, 2716: 9.0, 2755: 1.0, 2761: 1.0, 2764: 1.0, 2774: 4.0, 2900: 4.0, 2949: 2.0, 3039: 1.0, 3147: 1.0, 3249: 1.0, 3415: 1.0, 3435: 2.0, 3443: 1.0, 3458: 1.0, 3530: 1.0, 3628: 1.0, 3785: 1.0, 3837: 1.0, 4099: 1.0, 4200: 1.0, 4213: 1.0, 4276: 3.0, 4428: 1.0, 4527: 2.0, 4564: 4.0, 4577: 1.0, 4638: 4.0, 4645: 1.0, 4659: 2.0, 4670: 1.0, 4929: 6.0, 5115: 1.0, 5158: 1.0, 5293: 1.0, 5691: 1.0, 5861: 1.0, 5884: 1.0, 6054: 1.0, 6094: 3.0, 6638: 1.0, 6844: 1.0, 7188: 1.0, 7283: 1.0, 7349: 1.0, 7392: 1.0, 7948: 2.0, 8027: 1.0, 8201: 1.0, 8774: 1.0, 8843: 1.0, 8918: 1.0, 8936: 1.0, 9282: 1.0, 9325: 1.0, 9511: 1.0, 10002: 1.0, 10237: 1.0, 10606: 1.0, 11081: 1.0, 11587: 3.0, 11680: 1.0, 11683: 2.0, 11974: 1.0, 12216: 2.0, 12723: 1.0, 13143: 1.0, 13620: 1.0, 14270: 3.0, 15482: 1.0, 17052: 1.0, 17537: 1.0, 18095: 2.0, 18537: 1.0, 19096: 1.0, 19945: 1.0, 20545: 1.0, 21722: 1.0, 21989: 1.0, 22049: 1.0, 22793: 1.0, 23081: 1.0, 23951: 1.0, 24478: 1.0, 26078: 1.0, 26207: 1.0, 31347: 4.0, 34822: 2.0, 38049: 1.0, 39480: 1.0, 40750: 1.0, 43025: 1.0, 43822: 4.0, 45691: 1.0, 47126: 1.0, 49579: 2.0, 50997: 1.0, 55731: 5.0, 59949: 1.0, 62510: 1.0, 72886: 1.0, 90752: 1.0, 109100: 1.0, 111011: 1.0, 121775: 2.0, 137872: 1.0, 146825: 1.0, 171862: 1.0, 179565: 1.0, 191449: 1.0, 196125: 1.0, 205509: 1.0, 217605: 1.0, 240246: 1.0, 264393: 1.0, 297133: 1.0, 310887: 1.0, 329358: 1.0, 332271: 1.0, 349515: 1.0})],\n",
       " [5,\n",
       "  SparseVector(370239, {3: 1.0, 7: 1.0, 8: 1.0, 14: 1.0, 27: 1.0, 42: 1.0, 67: 2.0, 76: 1.0, 89: 3.0, 94: 1.0, 98: 1.0, 133: 1.0, 170: 1.0, 178: 1.0, 209: 1.0, 257: 1.0, 260: 2.0, 278: 1.0, 304: 2.0, 565: 1.0, 701: 1.0, 874: 1.0, 1021: 1.0, 1043: 2.0, 1169: 1.0, 1284: 1.0, 1314: 1.0, 1676: 1.0, 2573: 1.0, 3114: 1.0, 3596: 1.0, 3852: 1.0, 5155: 1.0, 10175: 1.0, 11836: 1.0, 111559: 1.0})],\n",
       " [6,\n",
       "  SparseVector(370239, {1: 1.0, 10: 1.0, 14: 1.0, 18: 1.0, 21: 1.0, 36: 1.0, 49: 1.0, 53: 1.0, 57: 1.0, 62: 1.0, 67: 2.0, 72: 1.0, 75: 1.0, 100: 1.0, 108: 1.0, 124: 1.0, 202: 2.0, 223: 1.0, 247: 1.0, 260: 2.0, 278: 1.0, 281: 1.0, 295: 1.0, 302: 1.0, 304: 2.0, 388: 1.0, 440: 1.0, 462: 1.0, 538: 2.0, 690: 1.0, 738: 1.0, 821: 1.0, 897: 1.0, 969: 1.0, 1043: 1.0, 1064: 1.0, 1081: 2.0, 1210: 1.0, 1431: 1.0, 1498: 1.0, 1738: 1.0, 1823: 1.0, 2795: 2.0, 3626: 1.0, 3953: 1.0, 4508: 1.0, 4830: 1.0, 5292: 1.0, 5537: 1.0, 13930: 1.0, 50919: 1.0})],\n",
       " [7,\n",
       "  SparseVector(370239, {14: 2.0, 124: 1.0, 157: 1.0, 306: 2.0, 346: 1.0, 392: 1.0, 514: 1.0, 541: 1.0, 544: 1.0, 697: 1.0, 1080: 1.0, 1508: 1.0, 1812: 1.0, 2331: 1.0, 2367: 1.0, 2516: 1.0, 2975: 1.0, 4488: 1.0, 5203: 1.0, 6841: 1.0})],\n",
       " [8,\n",
       "  SparseVector(370239, {7: 1.0, 28: 1.0, 34: 1.0, 37: 1.0, 44: 1.0, 69: 1.0, 105: 1.0, 118: 1.0, 164: 1.0, 178: 1.0, 239: 1.0, 242: 1.0, 257: 2.0, 260: 6.0, 278: 6.0, 304: 6.0, 350: 1.0, 436: 1.0, 448: 1.0, 554: 1.0, 697: 1.0, 824: 1.0, 1046: 1.0, 1139: 1.0, 1157: 1.0, 1521: 1.0, 1730: 1.0, 1846: 1.0, 1879: 1.0, 2005: 1.0, 2533: 2.0, 2706: 1.0, 3001: 1.0, 3070: 2.0, 4004: 1.0, 4380: 1.0, 5638: 2.0, 11559: 1.0, 12186: 1.0, 27954: 1.0, 29480: 1.0, 47586: 1.0})],\n",
       " [9,\n",
       "  SparseVector(370239, {8: 1.0, 45: 1.0, 46: 1.0, 53: 1.0, 273: 1.0, 346: 1.0, 482: 1.0, 958: 1.0, 972: 1.0, 1253: 1.0})]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('æ”¹åˆ°ä¸', 194994)\n"
     ]
    }
   ],
   "source": [
    "for a in vocabulary.items():\n",
    "    if a[1]==194994:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o291.trainLDAModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 854.0 failed 1 times, most recent failure: Lost task 5.0 in stage 854.0 (TID 470, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1088)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1082)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.computeGlobalTopicTotals(LDAOptimizer.scala:229)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:171)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:80)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:329)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLDAModel(PythonMLLibAPI.scala:552)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ac4dcbd6cc49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtopic_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribeTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxTermsPerTopic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words_per_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/clustering.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, rdd, k, maxIterations, docConcentration, topicConcentration, seed, checkpointInterval, optimizer)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         model = callMLlibFunc(\"trainLDAModel\", rdd, k, maxIterations,\n\u001b[1;32m   1038\u001b[0m                               \u001b[0mdocConcentration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicConcentration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                               checkpointInterval, optimizer)\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLDAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phejimlin/anaconda3/envs/spark/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o291.trainLDAModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 854.0 failed 1 times, most recent failure: Lost task 5.0 in stage 854.0 (TID 470, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1088)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1082)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.computeGlobalTopicTotals(LDAOptimizer.scala:229)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:171)\n\tat org.apache.spark.mllib.clustering.EMLDAOptimizer.initialize(LDAOptimizer.scala:80)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:329)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLDAModel(PythonMLLibAPI.scala:552)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n\tat scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n\tat org.apache.spark.graphx.impl.EdgePartitionBuilder.toEdgePartition(EdgePartitionBuilder.scala:43)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:112)\n\tat org.apache.spark.graphx.impl.GraphImpl$$anonfun$4.apply(GraphImpl.scala:106)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:843)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 47672)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 313, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 341, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 354, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/phejimlin/anaconda3/envs/spark/lib/python3.5/socketserver.py\", line 681, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 557, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/phejimlin/Documents/dcard_spark/output.txt\", 'w') as f:\n",
    "    lda_model = LDA.train(documents, k=num_topics)\n",
    "\n",
    "    topic_indices = lda_model.describeTopics(maxTermsPerTopic=num_words_per_topic)\n",
    "        \n",
    "    # Print topics, showing the top-weighted 10 terms for each topic\n",
    "    for i in range(len(topic_indices)):\n",
    "        f.write(\"Topic #{0}\\n\".format(i + 1))\n",
    "        for j in range(len(topic_indices[i][0])):\n",
    "            f.write(\"{0}\\t{1}\\n\".format(inv_voc[topic_indices[i][0][j]].encode('utf-8'), topic_indices[i][1][j]))\n",
    "            \n",
    "\n",
    "    f.write(\"{0} topics distributed over {1} documents and {2} unique words\\n\".format(topic_val, documents.count(), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
